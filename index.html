<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Beyond Accuracy</title>
  <link rel="icon" type="image/png" href="./assets/favicon/favicon-96x96.png" sizes="96x96" />
  <link rel="icon" type="image/svg+xml" href="./assets/favicon/favicon.svg" />
  <link rel="shortcut icon" href="./assets/favicon/favicon.ico" />
  <link rel="apple-touch-icon" sizes="180x180" href="./assets/favicon/apple-touch-icon.png"/>
  <link rel="manifest" href="./assets/favicon/site.webmanifest" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/react/18.2.0/umd/react.production.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/react-dom/18.2.0/umd/react-dom.production.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/plotly.js/2.27.1/plotly.min.js"></script>
  <style>
    /* Base Styles */
    body {
      font-family: Arial, sans-serif, "Segoe UI Emoji", "Apple Color Emoji", "Noto Color Emoji";
      margin: 0;
      padding: 0;
      display: flex;
      flex-direction: column;
      align-items: center;
      box-sizing: border-box;
      background-color: #fff;
    }
    .container {
      max-width: 70%;
      padding: 20px;
      box-sizing: border-box;
      width: 100%;
    }
    .controls-container {
      padding: 20px;
      box-sizing: border-box;
      width: 100%;
    }
    .header {
      text-align: center;
      margin-bottom: 30px;
      padding: 20px;
      background-color: #fff;
      box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    .title {
      font-size: clamp(24px, 5vw, 48px);
      font-weight: bold;
      margin: 0;
    }
    .subtitle {
      font-size: clamp(18px, 4vw, 36px);
      margin: 10px 0;
    }
    .conference {
      font-size: clamp(16px, 3.5vw, 32px);
      color: #666;
    }
    .authors {
      font-size: clamp(14px, 2.5vw, 24px);
      margin: 15px 0;
      display: flex;
      flex-wrap: wrap;
      justify-content: center;
      gap: 8px;
    }
    .affiliations {
      font-size: clamp(12px, 2vw, 20px);
      margin: 15px 0;
      display: flex;
      flex-wrap: wrap;
      justify-content: center;
      gap: 8px;
    }
    .note {
      font-size: clamp(12px, 2vw, 20px);
      margin: 15px 0;
    }
    .links {
      display: flex;
      justify-content: center;
      flex-wrap: wrap;
      gap: 15px;
      margin: 20px 0;
    }
    .link-button {
      background-color: #333;
      color: white;
      padding: 10px 20px;
      border-radius: 25px;
      text-decoration: none;
      display: flex;
      align-items: center;
      gap: 10px;
      font-size: clamp(12px, 1.8vw, 18px);
    }
    .controls {
      margin: 20px 0;
      background: #fff;
      padding: 20px;
      border-radius: 8px;
      box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    .control-group {
      margin-bottom: 20px;
    }
    .control-group label {
      font-weight: bold;
      margin-bottom: 10px;
      display: flex;
    }
    .checkbox-group, .radio-group {
      display: flex;
      flex-wrap: wrap;
      gap: 15px;
    }
    .checkbox-label, .radio-label {
      display: flex;
      align-items: center;
      gap: 5px;
      cursor: pointer;
    }
    .plot-container {
      width: 100%;
      overflow-x: auto;
    }
    .table-container {
      margin: 20px 0;
      width: 100%;
    }
    .data-table {
      width: 100%;
      border-collapse: collapse;
      font-size: clamp(12px, 1.5vw, 16px);
    }
    .data-table th, .data-table td {
      padding: 10px;
      text-align: left;
      border-bottom: 1px solid #ddd;
    }
    .data-table th {
      background-color: #f2f2f2;
      position: relative;
      cursor: pointer;
    }
    .data-table th:hover {
      background-color: #e0e0e0;
    }
    .data-table th.sort-asc::after {
      content: '‚ñ≤';
      position: absolute;
      right: 10px;
      top: 50%;
      transform: translateY(-50%);
    }
    .data-table th.sort-desc::after {
      content: '‚ñº';
      position: absolute;
      right: 10px;
      top: 50%;
      transform: translateY(-50%);
    }
    .reset-button {
      margin-top: 15px;
      padding: 10px 20px;
      background-color: #f44336;
      color: white;
      border: none;
      border-radius: 4px;
      cursor: pointer;
      font-size: clamp(12px, 1.5vw, 16px);
    }
    .reset-button:hover {
      background-color: #d32f2f;
    }
    .image-section {
        display: flex;
        justify-content: center; 
        align-items: center; 
        margin: 10px 0;
        margin-bottom: 20px; 
    }

    .image-section img {
        max-width: 60%;
        height: auto;
    }

    .project-description p, .citation p {
      font-size: clamp(14px, 1.8vw, 18px);
      line-height: 1.6;
      text-align: justify;
      margin: 10px 0; 
    }

    .abstract p {
      font-size: clamp(14px, 1.8vw, 18px);
      line-height: 1.6;
      text-align: justify;
      margin: 10px 0; 
    }

    .acknowledgements p {
      font-size: clamp(14px, 1.8vw, 18px);
      line-height: 1.6;
      text-align: justify;
      margin: 10px 0; 
    }

    .abstract h2 {
        text-align: center;
        font-size: clamp(26px, 2.2vw, 30px);
    }
    /* Media Queries for Responsiveness */
    @media (max-width: 768px) {
      .title {
        font-size: clamp(20px, 6vw, 36px);
      }
      .subtitle {
        font-size: clamp(16px, 5vw, 30px);
      }
      .conference {
        font-size: clamp(14px, 4vw, 28px);
      }
      .authors, .affiliations {
        font-size: clamp(12px, 3vw, 20px);
      }
      .links {
        flex-direction: column;
        align-items: center;
      }
      .link-button {
        width: 80%;
        justify-content: center;
      }
      .checkbox-group, .radio-group {
        flex-direction: column;
        gap: 10px;
      }
      .data-table {
        font-size: clamp(10px, 2vw, 14px);
      }
    }
  </style>
</head>
<body>
  <div class="container">
    <!-- Header Section -->
    <div class="header">
      <h1 class="title">Beyond Accuracy: What Matters in Designing Well-Behaved Models?</h1>
      <p class="subtitle">A Study on Model Behavior</p>
      <div class="authors">
        <span><a href="https://robinhesse.github.io/">Robin Hesse</a><sup>1*</sup></span>
        <span><a href="https://www.linkedin.com/in/dogukan-b-1b4a29211">Doƒüukan Baƒücƒ±</a><sup>1*</sup></span>
        <span><a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a><sup>2</sup></span>
        <span><a href="https://schaubsi.github.io/">Simone Schaub-Meyer</a><sup>1,3</sup></span>
        <span><a href="https://www.visinf.tu-darmstadt.de/visual_inference/people_vi/stefan_roth.en.jsp">Stefan Roth</a><sup>1,3</sup></span>
      </div>
      <div class="affiliations">
        <span><a href='https://www.visinf.tu-darmstadt.de/research_vi/index.en.jsp'>Technical University of Darmstadt</a><sup>1</sup></span>
        <span><a href='https://www.mpi-inf.mpg.de/publications'>Max Planck Institute for Informatics, SIC</a><sup>2</sup></span>
        <span><a href='https://hessian.ai/research/'>hessian.AI</a><sup>3</sup></span>

      </div>
      <p class="note"><sup>*</sup>equal contribution</p>
      <div class="links">
        <a href="https://arxiv.org/abs/2503.17110" class="link-button"><span>üìù</span>Paper</a>
        <a href="https://github.com/visinf/beyond-accuracy/tree/main" class="link-button"><span>üíª</span>Code</a>
        <a href="https://github.com/visinf/beyond-accuracy/tree/main#how-to-add-and-test-your-own-model" class="link-button">
            <img 
                src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png" 
                alt="GitHub" 
                width="20" 
                height="20" 
            />
            How to evaluate your model?
        </a>
      </div>
    </div>

    <!-- Image Section -->
    <div class="image-section">
      <img src="./assets/teaser_figure_website.png" alt="Teaser Image">
    </div>

    <div class="abstract" text-align=center>
      <h2>Abstract</h2>
      <p>Deep learning has become an essential part of computer vision, with deep neural networks (DNNs) excelling in predictive performance. However, they often fall short in other critical quality dimensions, such as robustness, calibration, or fairness. While existing studies have focused on a subset of these quality dimensions, none have explored a more general form of "well-behavedness" of DNNs. With this work, we address this gap by simultaneously studying nine different quality dimensions for image classification. Through a large-scale study, we provide a bird's-eye view by analyzing 326 models backbone models and how different training paradigms and model architectures affect the quality dimensions. We reveal various new insights such that (i) vision-language models exhibit high fairness on ImageNet-1k classification and strong robustness against domain changes; (ii) self-supervised learning is an effective training paradigm to improve almost all considered quality dimensions; and (iii) the training dataset size is a major driver for most of the quality dimensions. We conclude our study by introducing the QUBA score (Quality Understanding Beyond Accuracy), a novel metric that ranks models across multiple dimensions of quality, enabling tailored recommendations based on specific user needs.</p>
    </div>

    <br> </br>

    <!-- Interactive Plot Section -->
    <div class="project-description">
      <h2>Interactive Plot & Table</h2>
      <p>Welcome to our interactive plot! Here, you can explore the data used in our analysis to gain deeper insights into specific models or groups of models. Use the checkboxes below the scatterplot to select the subsets you'd like to visualize. You can filter by (pre-)training datasets, architectures, and various training paradigms. The selected models will be displayed in a table located below the checkboxes. By default, the table is hidden, but you can reveal it by clicking the 'Show Table' button. Also, you can customize the scatterplot by selecting which quality dimensions to display on the x- and y-axes.</p>
    </div>
    <div class="plot-container">
      <div id="plot"></div>
    </div>

    <div id="root"></div>
    <script>
      const e = React.createElement;
  
      const ALL_ARCHITECTURES = [
      ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
      ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
      ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#59a14f" stroke="#59a14f" stroke-width="1" opacity="0.7"/></svg> Bcos models',
      ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models'
      ];
  
      const AXIS_RANGES = {
          "Accuracy": [0, 1],
          "Adversarial Robustness": [0, 1],
          "Corruption Robustness": [0, 1],
          "OOD Robustness": [0, 1.2],
          "Calibration Error": [0, 0.05],
          "Class Balance": [0, 1],
          "Object Focus": [0, 1],
          "Shape Bias": [0, 1],
          "Parameters": [0, 500],
      };
  
      const ALL_DATASETS = [
          " ImageNet-1k",
          " ‚óè ImageNet-21k",
          " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)"
      ];
  
      const ALL_TRAINING = [
          " ‚ñ† Supervised learning",
          " ‚óè Adversarial training",
          " ‚ñ≤ Self-supervised learning",
          " ‚óÜ Semi-supervised learning",
          " ‚¨ü A[1,2,3]"
      ];
  
      const ALL_METRICS = [
          "Accuracy",
          "Adversarial Robustness",
          "Corruption Robustness",
          "OOD Robustness",
          "Calibration Error",
          "Class Balance",
          "Object Focus",
          "Shape Bias",
          "Parameters",
      ];
  
      const SHAPE_MAP = {
          " ‚ñ† Supervised learning": "square",
          " ‚óè Adversarial training": "circle",
          " ‚ñ≤ Self-supervised learning": "triangle-up",
          " ‚óÜ Semi-supervised learning": "diamond",
          " ‚¨ü A[1,2,3]": "pentagon"
      };
  
      const COLOR_MAP = {
          ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN': "#4e79a7",
          ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer': "#f28e2b",
          ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#59a14f" stroke="#59a14f" stroke-width="1" opacity="0.7"/></svg> Bcos models': "#59a14f",
          ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models': "#edc948"
      };
  
      const App = () => {
          const [data, setData] = React.useState([]);
          const [filters, setFilters] = React.useState({
              architectures: [...ALL_ARCHITECTURES],
              datasets: [...ALL_DATASETS],
              training: [...ALL_TRAINING],
              xAxis: "Accuracy",
              yAxis: "Adversarial Robustness"
          });
  
          React.useEffect(() => {

              const sampleData = [
              {
              Model: "AlexNet",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "ImageNet classification with deep convolutional neural networks",
              Accuracy: 0.5652,
              "Adversarial Robustness": 0.0352,
              "Corruption Robustness": 0.3442,
              "OOD Robustness": 0.5699,
              "Calibration Error": 0.0017,
              "Class Balance": 0.7319,
              "Object Focus": 0.8,
              "Shape Bias": 0.2636,
              Parameters: 61.1,
          },
  
          
          {
              Model: "GoogLeNet",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Going deeper with convolutions",
              Accuracy: 0.6978,
              "Adversarial Robustness": 0.0834,
              "Corruption Robustness": 0.5012,
              "OOD Robustness": 0.544,
              "Calibration Error": 0.0032,
              "Class Balance": 0.7482,
              "Object Focus": 0.914,
              "Shape Bias": 0.2506,
              Parameters: 6.6,
          },
          
  
          
          {
              Model: "VGG11",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Very Deep Convolutional Networks for Large-Scale Image Recognition",
              Accuracy: 0.6902,
              "Adversarial Robustness": 0.0231,
              "Corruption Robustness": 0.378,
              "OOD Robustness": 0.4616,
              "Calibration Error": 0.0014,
              "Class Balance": 0.7415,
              "Object Focus": 0.8616,
              "Shape Bias": 0.1053,
              Parameters: 132.9,
          },
          
  
          
          {
              Model: "VGG13",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Very Deep Convolutional Networks for Large-Scale Image Recognition",
              Accuracy: 0.6993,
              "Adversarial Robustness": 0.0228,
              "Corruption Robustness": 0.3689,
              "OOD Robustness": 0.4406,
              "Calibration Error": 0.0015,
              "Class Balance": 0.7427,
              "Object Focus": 0.8648,
              "Shape Bias": 0.109,
              Parameters: 133.0,
          },
          
  
          
          {
              Model: "VGG16",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Very Deep Convolutional Networks for Large-Scale Image Recognition",
              Accuracy: 0.7159,
              "Adversarial Robustness": 0.0236,
              "Corruption Robustness": 0.3994,
              "OOD Robustness": 0.4283,
              "Calibration Error": 0.0017,
              "Class Balance": 0.7451,
              "Object Focus": 0.8617,
              "Shape Bias": 0.1075,
              Parameters: 138.4,
          },
          
  
          
          {
              Model: "VGG19",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Very Deep Convolutional Networks for Large-Scale Image Recognition",
              Accuracy: 0.7237,
              "Adversarial Robustness": 0.0266,
              "Corruption Robustness": 0.4111,
              "OOD Robustness": 0.4449,
              "Calibration Error": 0.0017,
              "Class Balance": 0.7462,
              "Object Focus": 0.8683,
              "Shape Bias": 0.1244,
              Parameters: 143.7,
          },
          
  
          
          {
              Model: "VGG11-bn",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Very Deep Convolutional Networks for Large-Scale Image Recognition",
              Accuracy: 0.7038,
              "Adversarial Robustness": 0.0176,
              "Corruption Robustness": 0.4155,
              "OOD Robustness": 0.4983,
              "Calibration Error": 0.0017,
              "Class Balance": 0.7426,
              "Object Focus": 0.8695,
              "Shape Bias": 0.1055,
              Parameters: 132.9,
          },
          
  
          
          {
              Model: "VGG13-bn",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Very Deep Convolutional Networks for Large-Scale Image Recognition",
              Accuracy: 0.7159,
              "Adversarial Robustness": 0.0163,
              "Corruption Robustness": 0.4023,
              "OOD Robustness": 0.4778,
              "Calibration Error": 0.0017,
              "Class Balance": 0.7451,
              "Object Focus": 0.8803,
              "Shape Bias": 0.1152,
              Parameters: 133.1,
          },
          
  
          
          {
              Model: "VGG16-bn",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Very Deep Convolutional Networks for Large-Scale Image Recognition",
              Accuracy: 0.7337,
              "Adversarial Robustness": 0.0189,
              "Corruption Robustness": 0.452,
              "OOD Robustness": 0.4796,
              "Calibration Error": 0.002,
              "Class Balance": 0.7479,
              "Object Focus": 0.892,
              "Shape Bias": 0.1129,
              Parameters: 138.4,
          },
          
  
          
          {
              Model: "VGG19-bn",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Very Deep Convolutional Networks for Large-Scale Image Recognition",
              Accuracy: 0.7422,
              "Adversarial Robustness": 0.022,
              "Corruption Robustness": 0.4776,
              "OOD Robustness": 0.5012,
              "Calibration Error": 0.0021,
              "Class Balance": 0.7504,
              "Object Focus": 0.8942,
              "Shape Bias": 0.1532,
              Parameters: 143.7,
          },
          
  
          
          {
              Model: "ResNet18",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Deep Residual Learning for Image Recognition",
              Accuracy: 0.6975,
              "Adversarial Robustness": 0.0257,
              "Corruption Robustness": 0.472,
              "OOD Robustness": 0.559,
              "Calibration Error": 0.0018,
              "Class Balance": 0.7398,
              "Object Focus": 0.8793,
              "Shape Bias": 0.2367,
              Parameters: 11.7,
          },
          
  
          
          {
              Model: "ResNet34",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Deep Residual Learning for Image Recognition",
              Accuracy: 0.7331,
              "Adversarial Robustness": 0.0336,
              "Corruption Robustness": 0.5215,
              "OOD Robustness": 0.5461,
              "Calibration Error": 0.002,
              "Class Balance": 0.7465,
              "Object Focus": 0.9016,
              "Shape Bias": 0.2595,
              Parameters: 21.8,
          },
          
  
          
          {
              Model: "ResNet50",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Deep Residual Learning for Image Recognition",
              Accuracy: 0.7613,
              "Adversarial Robustness": 0.0337,
              "Corruption Robustness": 0.5146,
              "OOD Robustness": 0.4968,
              "Calibration Error": 0.0021,
              "Class Balance": 0.7549,
              "Object Focus": 0.9274,
              "Shape Bias": 0.2221,
              Parameters: 25.6,
          },
          
  
          
          {
              Model: "ResNet101",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Deep Residual Learning for Image Recognition",
              Accuracy: 0.7738,
              "Adversarial Robustness": 0.0442,
              "Corruption Robustness": 0.5699,
              "OOD Robustness": 0.5242,
              "Calibration Error": 0.0023,
              "Class Balance": 0.7585,
              "Object Focus": 0.9215,
              "Shape Bias": 0.2866,
              Parameters: 44.5,
          },
          
  
          
          {
              Model: "ResNet152",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Deep Residual Learning for Image Recognition",
              Accuracy: 0.7832,
              "Adversarial Robustness": 0.0564,
              "Corruption Robustness": 0.574,
              "OOD Robustness": 0.55,
              "Calibration Error": 0.0023,
              "Class Balance": 0.7624,
              "Object Focus": 0.9426,
              "Shape Bias": 0.294,
              Parameters: 60.2,
          },
          
  
          
          {
              Model: "WRN-50-2",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Wide Residual Networks",
              Accuracy: 0.7848,
              "Adversarial Robustness": 0.0492,
              "Corruption Robustness": 0.5385,
              "OOD Robustness": 0.4669,
              "Calibration Error": 0.0023,
              "Class Balance": 0.7605,
              "Object Focus": 0.9365,
              "Shape Bias": 0.2304,
              Parameters: 68.9,
          },
          
  
          
          {
              Model: "WRN-101-2",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Wide Residual Networks",
              Accuracy: 0.7884,
              "Adversarial Robustness": 0.0542,
              "Corruption Robustness": 0.5734,
              "OOD Robustness": 0.541,
              "Calibration Error": 0.0023,
              "Class Balance": 0.7617,
              "Object Focus": 0.9358,
              "Shape Bias": 0.2826,
              Parameters: 126.9,
          },
          
  
          
          {
              Model: "SqueezeNet",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and less than 1MB",
              Accuracy: 0.5818,
              "Adversarial Robustness": 0.0145,
              "Corruption Robustness": 0.3153,
              "OOD Robustness": 0.4765,
              "Calibration Error": 0.0013,
              "Class Balance": 0.7352,
              "Object Focus": 0.7713,
              "Shape Bias": 0.2135,
              Parameters: 1.2,
          },
          
  
          
          {
              Model: "InceptionV3",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Rethinking the Inception Architecture for Computer Vision",
              Accuracy: 0.7729,
              "Adversarial Robustness": 0.1552,
              "Corruption Robustness": 0.5576,
              "OOD Robustness": 0.5481,
              "Calibration Error": 0.0017,
              "Class Balance": 0.7603,
              "Object Focus": 0.9387,
              "Shape Bias": 0.2954,
              Parameters: 27.2,
          },
          
  
          
          {
              Model: "InceptionV4",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
              Accuracy: 0.8013,
              "Adversarial Robustness": 0.0818,
              "Corruption Robustness": 0.6103,
              "OOD Robustness": 0.5436,
              "Calibration Error": 0.0014,
              "Class Balance": 0.774,
              "Object Focus": 0.9396,
              "Shape Bias": 0.2388,
              Parameters: 42.7,
          },
          
  
          
          {
              Model: "Inception-ResNetv2",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
              Accuracy: 0.8045,
              "Adversarial Robustness": 0.1513,
              "Corruption Robustness": 0.6316,
              "OOD Robustness": 0.5563,
              "Calibration Error": 0.0022,
              "Class Balance": 0.7799,
              "Object Focus": 0.9484,
              "Shape Bias": 0.2641,
              Parameters: 55.8,
          },
          
  
          
          {
              Model: "ResNeXt50-32x4d",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Aggregated Residual Transformations for Deep Neural Networks",
              Accuracy: 0.7761,
              "Adversarial Robustness": 0.0412,
              "Corruption Robustness": 0.5414,
              "OOD Robustness": 0.528,
              "Calibration Error": 0.0026,
              "Class Balance": 0.7556,
              "Object Focus": 0.9348,
              "Shape Bias": 0.2189,
              Parameters: 25.0,
          },
          
  
          
          {
              Model: "ResNeXt101-32x8d",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Aggregated Residual Transformations for Deep Neural Networks",
              Accuracy: 0.7931,
              "Adversarial Robustness": 0.069,
              "Corruption Robustness": 0.5868,
              "OOD Robustness": 0.5541,
              "Calibration Error": 0.0029,
              "Class Balance": 0.7613,
              "Object Focus": 0.9427,
              "Shape Bias": 0.2936,
              Parameters: 88.8,
          },
          
  
          
          {
              Model: "ResNeXt101-64x4d",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Aggregated Residual Transformations for Deep Neural Networks",
              Accuracy: 0.8325,
              "Adversarial Robustness": 0.3463,
              "Corruption Robustness": 0.6912,
              "OOD Robustness": 0.6087,
              "Calibration Error": 0.0196,
              "Class Balance": 0.8455,
              "Object Focus": 0.9502,
              "Shape Bias": 0.2435,
              Parameters: 83.5,
          },
          
  
          
          {
              Model: "DenseNet121",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Densely Connected Convolutional Networks",
              Accuracy: 0.7444,
              "Adversarial Robustness": 0.0556,
              "Corruption Robustness": 0.5364,
              "OOD Robustness": 0.5139,
              "Calibration Error": 0.0017,
              "Class Balance": 0.7526,
              "Object Focus": 0.9194,
              "Shape Bias": 0.2221,
              Parameters: 8.0,
          },
          
  
          
          {
              Model: "DenseNet161",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Densely Connected Convolutional Networks",
              Accuracy: 0.7711,
              "Adversarial Robustness": 0.0778,
              "Corruption Robustness": 0.5941,
              "OOD Robustness": 0.5669,
              "Calibration Error": 0.0025,
              "Class Balance": 0.758,
              "Object Focus": 0.9404,
              "Shape Bias": 0.2788,
              Parameters: 28.7,
          },
          
  
          
          {
              Model: "DenseNet169",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Densely Connected Convolutional Networks",
              Accuracy: 0.7558,
              "Adversarial Robustness": 0.0694,
              "Corruption Robustness": 0.5708,
              "OOD Robustness": 0.5646,
              "Calibration Error": 0.0025,
              "Class Balance": 0.7523,
              "Object Focus": 0.9285,
              "Shape Bias": 0.2628,
              Parameters: 14.1,
          },
          
  
          
          {
              Model: "DenseNet201",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Densely Connected Convolutional Networks",
              Accuracy: 0.7689,
              "Adversarial Robustness": 0.079,
              "Corruption Robustness": 0.5724,
              "OOD Robustness": 0.5875,
              "Calibration Error": 0.0019,
              "Class Balance": 0.7585,
              "Object Focus": 0.947,
              "Shape Bias": 0.2686,
              Parameters: 20.0,
          },
          
  
          
          {
              Model: "Xception",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Xception: Deep Learning with Depthwise Separable Convolutions",
              Accuracy: 0.7904,
              "Adversarial Robustness": 0.0541,
              "Corruption Robustness": 0.5737,
              "OOD Robustness": 0.5219,
              "Calibration Error": 0.0031,
              "Class Balance": 0.7682,
              "Object Focus": 0.9322,
              "Shape Bias": 0.2159,
              Parameters: 22.9,
          },
          
  
          
          {
              Model: "MobileNetV2",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
              Accuracy: 0.7187,
              "Adversarial Robustness": 0.022,
              "Corruption Robustness": 0.4351,
              "OOD Robustness": 0.4968,
              "Calibration Error": 0.0018,
              "Class Balance": 0.7454,
              "Object Focus": 0.8791,
              "Shape Bias": 0.1805,
              Parameters: 3.5,
          },
          
  
          
          {
              Model: "ShuffleNet-v2-05",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "ShuffleNet V2: Practical Guidelines for Efficient {CNN} Architecture Design",
              Accuracy: 0.6056,
              "Adversarial Robustness": 0.0108,
              "Corruption Robustness": 0.3578,
              "OOD Robustness": 0.4764,
              "Calibration Error": 0.0028,
              "Class Balance": 0.7217,
              "Object Focus": 0.8475,
              "Shape Bias": 0.267,
              Parameters: 1.4,
          },
          
  
          
          {
              Model: "ShuffleNet-v2-1",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "ShuffleNet V2: Practical Guidelines for Efficient {CNN} Architecture Design",
              Accuracy: 0.6933,
              "Adversarial Robustness": 0.0143,
              "Corruption Robustness": 0.4126,
              "OOD Robustness": 0.4876,
              "Calibration Error": 0.0031,
              "Class Balance": 0.7291,
              "Object Focus": 0.8846,
              "Shape Bias": 0.2318,
              Parameters: 2.3,
          },
          
  
          
          {
              Model: "ShuffleNet-v2-15",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "ShuffleNet V2: Practical Guidelines for Efficient {CNN} Architecture Design",
              Accuracy: 0.7299,
              "Adversarial Robustness": 0.044,
              "Corruption Robustness": 0.5145,
              "OOD Robustness": 0.5675,
              "Calibration Error": 0.0143,
              "Class Balance": 0.7836,
              "Object Focus": 0.8874,
              "Shape Bias": 0.2131,
              Parameters: 3.5,
          },
          
  
          
          {
              Model: "ShuffleNet-v2-2",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "ShuffleNet V2: Practical Guidelines for Efficient {CNN} Architecture Design",
              Accuracy: 0.7623,
              "Adversarial Robustness": 0.0547,
              "Corruption Robustness": 0.5465,
              "OOD Robustness": 0.5676,
              "Calibration Error": 0.0133,
              "Class Balance": 0.7841,
              "Object Focus": 0.9151,
              "Shape Bias": 0.2421,
              Parameters: 7.4,
          },
          
  
          
          {
              Model: "NasNet-l",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Learning Transferable Architectures for Scalable Image Recognition",
              Accuracy: 0.8263,
              "Adversarial Robustness": 0.189,
              "Corruption Robustness": 0.6757,
              "OOD Robustness": 0.5471,
              "Calibration Error": 0.0029,
              "Class Balance": 0.7948,
              "Object Focus": 0.9381,
              "Shape Bias": 0.2392,
              Parameters: 88.8,
          },
          
  
          
          {
              Model: "MobileNetV3-s",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Searching for MobileNetV3",
              Accuracy: 0.6767,
              "Adversarial Robustness": 0.0272,
              "Corruption Robustness": 0.4869,
              "OOD Robustness": 0.5554,
              "Calibration Error": 0.0017,
              "Class Balance": 0.737,
              "Object Focus": 0.8373,
              "Shape Bias": 0.3189,
              Parameters: 2.5,
          },
          
  
          
          {
              Model: "MobileNetV3-l",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Searching for MobileNetV4",
              Accuracy: 0.7405,
              "Adversarial Robustness": 0.0314,
              "Corruption Robustness": 0.5228,
              "OOD Robustness": 0.5785,
              "Calibration Error": 0.0026,
              "Class Balance": 0.7455,
              "Object Focus": 0.8656,
              "Shape Bias": 0.2648,
              Parameters: 5.5,
          },
          
  
          
          {
              Model: "MobileNetV3-l",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ† Supervised learning",
              Text: "‚Ä¢",
              Title: "Searching for MobileNetV5",
              Accuracy: 0.779,
              "Adversarial Robustness": 0.0026,
              "Corruption Robustness": 0.5557,
              "OOD Robustness": 0.5402,
              "Calibration Error": 0.002,
              "Class Balance": 0.7605,
              "Object Focus": 0.8843,
              "Shape Bias": 0.2699,
              Parameters: 5.5,
          },
          
  
          
          {
              Model: "BagNet9",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Approximating CNNs with Bag-of-local-Features models works surprisingly",
              Accuracy: 0.4579,
              "Adversarial Robustness": 0.0035,
              "Corruption Robustness": 0.1547,
              "OOD Robustness": 0.2473,
              "Calibration Error": 0.0052,
              "Class Balance": 0.7362,
              "Object Focus": 0.8053,
              "Shape Bias": 0.039,
              Parameters: 15.7,
          },
          
  
          
          {
              Model: "BagNet17",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Approximating CNNs with Bag-of-local-Features models works surprisingly",
              Accuracy: 0.5844,
              "Adversarial Robustness": 0.0088,
              "Corruption Robustness": 0.2179,
              "OOD Robustness": 0.2786,
              "Calibration Error": 0.0042,
              "Class Balance": 0.7195,
              "Object Focus": 0.7892,
              "Shape Bias": 0.0292,
              Parameters: 16.2,
          },
          
  
          
          {
              Model: "BagNet33",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Approximating CNNs with Bag-of-local-Features models works surprisingly",
              Accuracy: 0.6453,
              "Adversarial Robustness": 0.0122,
              "Corruption Robustness": 0.2786,
              "OOD Robustness": 0.3059,
              "Calibration Error": 0.0041,
              "Class Balance": 0.7211,
              "Object Focus": 0.8504,
              "Shape Bias": 0.0453,
              Parameters: 18.3,
          },
          
  
          
          {
              Model: "MnasNet-05",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "MnasNet: Platform-Aware Neural Architecture Search for Mobile",
              Accuracy: 0.6776,
              "Adversarial Robustness": 0.0238,
              "Corruption Robustness": 0.4001,
              "OOD Robustness": 0.4793,
              "Calibration Error": 0.0041,
              "Class Balance": 0.7498,
              "Object Focus": 0.8389,
              "Shape Bias": 0.2346,
              Parameters: 2.2,
          },
          
  
          
          {
              Model: "MnasNet-075",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "MnasNet: Platform-Aware Neural Architecture Search for Mobile",
              Accuracy: 0.7117,
              "Adversarial Robustness": 0.0574,
              "Corruption Robustness": 0.4681,
              "OOD Robustness": 0.531,
              "Calibration Error": 0.0185,
              "Class Balance": 0.8054,
              "Object Focus": 0.8768,
              "Shape Bias": 0.2069,
              Parameters: 3.2,
          },
          
  
          
          {
              Model: "MnasNet-1",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "MnasNet: Platform-Aware Neural Architecture Search for Mobile",
              Accuracy: 0.7347,
              "Adversarial Robustness": 0.0247,
              "Corruption Robustness": 0.4498,
              "OOD Robustness": 0.5121,
              "Calibration Error": 0.0022,
              "Class Balance": 0.7474,
              "Object Focus": 0.8783,
              "Shape Bias": 0.2224,
              Parameters: 4.4,
          },
          
  
          
          {
              Model: "MnasNet-13",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "MnasNet: Platform-Aware Neural Architecture Search for Mobile",
              Accuracy: 0.7651,
              "Adversarial Robustness": 0.1281,
              "Corruption Robustness": 0.5276,
              "OOD Robustness": 0.5122,
              "Calibration Error": 0.0187,
              "Class Balance": 0.8122,
              "Object Focus": 0.9061,
              "Shape Bias": 0.1933,
              Parameters: 6.3,
          },
          
  
          
          {
              Model: "EfficientNet-B0",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
              Accuracy: 0.7769,
              "Adversarial Robustness": 0.0722,
              "Corruption Robustness": 0.7706,
              "OOD Robustness": 0.5961,
              "Calibration Error": 0.0034,
              "Class Balance": 0.7698,
              "Object Focus": 0.8948,
              "Shape Bias": 0.2488,
              Parameters: 5.3,
          },
          
  
          
          {
              Model: "EfficientNet-B1",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
              Accuracy: 0.7864,
              "Adversarial Robustness": 0.1403,
              "Corruption Robustness": 0.6337,
              "OOD Robustness": 0.5811,
              "Calibration Error": 0.0049,
              "Class Balance": 0.7831,
              "Object Focus": 0.9017,
              "Shape Bias": 0.2414,
              Parameters: 7.8,
          },
          
  
          
          {
              Model: "EfficientNet-B2",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
              Accuracy: 0.8061,
              "Adversarial Robustness": 0.1392,
              "Corruption Robustness": 0.5968,
              "OOD Robustness": 0.6271,
              "Calibration Error": 0.0041,
              "Class Balance": 0.7861,
              "Object Focus": 0.9005,
              "Shape Bias": 0.2063,
              Parameters: 9.1,
          },
          
  
          
          {
              Model: "EfficientNet-B3",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
              Accuracy: 0.8201,
              "Adversarial Robustness": 0.222,
              "Corruption Robustness": 0.5327,
              "OOD Robustness": 0.7047,
              "Calibration Error": 0.0065,
              "Class Balance": 0.8018,
              "Object Focus": 0.9287,
              "Shape Bias": 0.2734,
              Parameters: 12.2,
          },
          
  
          
          {
              Model: "EfficientNet-B4",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
              Accuracy: 0.8338,
              "Adversarial Robustness": 0.2449,
              "Corruption Robustness": 0.5616,
              "OOD Robustness": 0.7588,
              "Calibration Error": 0.0132,
              "Class Balance": 0.8272,
              "Object Focus": 0.9272,
              "Shape Bias": 0.2053,
              Parameters: 19.3,
          },
          
  
          
          {
              Model: "EfficientNet-B5",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
              Accuracy: 0.8345,
              "Adversarial Robustness": 0.1947,
              "Corruption Robustness": 0.5542,
              "OOD Robustness": 0.5863,
              "Calibration Error": 0.0054,
              "Class Balance": 0.8088,
              "Object Focus": 0.9213,
              "Shape Bias": 0.2483,
              Parameters: 30.4,
          },
          
  
          
          {
              Model: "EfficientNet-B6",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
              Accuracy: 0.8401,
              "Adversarial Robustness": 0.2109,
              "Corruption Robustness": 0.5918,
              "OOD Robustness": 0.5475,
              "Calibration Error": 0.0065,
              "Class Balance": 0.8159,
              "Object Focus": 0.923,
              "Shape Bias": 0.2337,
              Parameters: 43.0,
          },
          
  
          
          {
              Model: "EfficientNet-B7",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
              Accuracy: 0.8412,
              "Adversarial Robustness": 0.2199,
              "Corruption Robustness": 0.6669,
              "OOD Robustness": 0.5739,
              "Calibration Error": 0.0061,
              "Class Balance": 0.8149,
              "Object Focus": 0.9184,
              "Shape Bias": 0.2599,
              Parameters: 66.3,
          },
          
  
          
          {
              Model: "EfficientNet-B0",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚óÜ Semi-supervised learning",
              Text: "‚òÖ",
              Title: "Self-Training With Noisy Student Improves ImageNet Classification",
              Accuracy: 0.7867,
              "Adversarial Robustness": 0.1098,
              "Corruption Robustness": 0.5695,
              "OOD Robustness": 0.6645,
              "Calibration Error": 0.0037,
              "Class Balance": 0.7784,
              "Object Focus": 0.893,
              "Shape Bias": 0.2354,
              Parameters: 5.3,
          },
          
  
          
          {
              Model: "EfficientNet-B1",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚óÜ Semi-supervised learning",
              Text: "‚òÖ",
              Title: "Self-Training With Noisy Student Improves ImageNet Classification",
              Accuracy: 0.8139,
              "Adversarial Robustness": 0.147,
              "Corruption Robustness": 0.6155,
              "OOD Robustness": 0.7158,
              "Calibration Error": 0.0043,
              "Class Balance": 0.7942,
              "Object Focus": 0.9211,
              "Shape Bias": 0.2512,
              Parameters: 7.8,
          },
          
  
          
          {
              Model: "EfficientNet-B2",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚óÜ Semi-supervised learning",
              Text: "‚òÖ",
              Title: "Self-Training With Noisy Student Improves ImageNet Classification",
              Accuracy: 0.8239,
              "Adversarial Robustness": 0.1651,
              "Corruption Robustness": 0.6377,
              "OOD Robustness": 0.7226,
              "Calibration Error": 0.0044,
              "Class Balance": 0.7992,
              "Object Focus": 0.9254,
              "Shape Bias": 0.238,
              Parameters: 9.1,
          },
          
  
          
          {
              Model: "EfficientNet-B3",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚óÜ Semi-supervised learning",
              Text: "‚òÖ",
              Title: "Self-Training With Noisy Student Improves ImageNet Classification",
              Accuracy: 0.8406,
              "Adversarial Robustness": 0.1773,
              "Corruption Robustness": 0.6742,
              "OOD Robustness": 0.7715,
              "Calibration Error": 0.0043,
              "Class Balance": 0.8069,
              "Object Focus": 0.9269,
              "Shape Bias": 0.2725,
              Parameters: 12.2,
          },
          
  
          
          {
              Model: "EfficientNet-B4",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚óÜ Semi-supervised learning",
              Text: "‚òÖ",
              Title: "Self-Training With Noisy Student Improves ImageNet Classification",
              Accuracy: 0.8515,
              "Adversarial Robustness": 0.1979,
              "Corruption Robustness": 0.7214,
              "OOD Robustness": 0.7729,
              "Calibration Error": 0.005,
              "Class Balance": 0.8142,
              "Object Focus": 0.9226,
              "Shape Bias": 0.2773,
              Parameters: 19.3,
          },
          
  
          
          {
              Model: "EfficientNet-B5",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚óÜ Semi-supervised learning",
              Text: "‚òÖ",
              Title: "Self-Training With Noisy Student Improves ImageNet Classification",
              Accuracy: 0.8608,
              "Adversarial Robustness": 0.2016,
              "Corruption Robustness": 0.7568,
              "OOD Robustness": 0.7969,
              "Calibration Error": 0.0049,
              "Class Balance": 0.8195,
              "Object Focus": 0.9386,
              "Shape Bias": 0.3099,
              Parameters: 30.4,
          },
          
  
          
          {
              Model: "EfficientNet-B6",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚óÜ Semi-supervised learning",
              Text: "‚òÖ",
              Title: "Self-Training With Noisy Student Improves ImageNet Classification",
              Accuracy: 0.8645,
              "Adversarial Robustness": 0.2453,
              "Corruption Robustness": 0.7732,
              "OOD Robustness": 0.8335,
              "Calibration Error": 0.0048,
              "Class Balance": 0.8235,
              "Object Focus": 0.9452,
              "Shape Bias": 0.3549,
              Parameters: 43.0,
          },
          
  
          
          {
              Model: "EfficientNet-B7",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚óÜ Semi-supervised learning",
              Text: "‚òÖ",
              Title: "Self-Training With Noisy Student Improves ImageNet Classification",
              Accuracy: 0.8683,
              "Adversarial Robustness": 0.2944,
              "Corruption Robustness": 0.7742,
              "OOD Robustness": 0.8551,
              "Calibration Error": 0.0064,
              "Class Balance": 0.8292,
              "Object Focus": 0.9463,
              "Shape Bias": 0.4409,
              Parameters: 66.3,
          },
          
  
          
          {
              Model: "WRN-50-2",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚óè Adversarial training",
              Text: "",
              Title: "Do Adversarially Robust ImageNet Models Transfer Better?",
              Accuracy: 0.684,
              "Adversarial Robustness": 0.3763,
              "Corruption Robustness": 0.5159,
              "OOD Robustness": 0.472,
              "Calibration Error": 0.0027,
              "Class Balance": 0.7427,
              "Object Focus": 0.9069,
              "Shape Bias": 0.6671,
              Parameters: 68.9,
          },
          
  
          
          {
              Model: "ResNet50",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚óè Adversarial training",
              Text: "",
              Title: "Do Adversarially Robust ImageNet Models Transfer Better?",
              Accuracy: 0.6386,
              "Adversarial Robustness": 0.3626,
              "Corruption Robustness": 0.5016,
              "OOD Robustness": 0.4111,
              "Calibration Error": 0.0037,
              "Class Balance": 0.7449,
              "Object Focus": 0.8775,
              "Shape Bias": 0.6429,
              Parameters: 25.6,
          },
          
  
          
          {
              Model: "BiTM-ResNet50x1",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ† Supervised learning",
              Text: "‚Ä¢",
              Title: "Big Transfer (BiT): General Visual Representation Learning",
              Accuracy: 0.8034,
              "Adversarial Robustness": 0.0121,
              "Corruption Robustness": 0.5526,
              "OOD Robustness": 0.653,
              "Calibration Error": 0.0012,
              "Class Balance": 0.7781,
              "Object Focus": 0.9133,
              "Shape Bias": 0.1851,
              Parameters: 25.5,
          },
          
  
          
          {
              Model: "BiTM-ResNet50x3",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ† Supervised learning",
              Text: "‚Ä¢",
              Title: "Big Transfer (BiT): General Visual Representation Learning",
              Accuracy: 0.8402,
              "Adversarial Robustness": 0.041,
              "Corruption Robustness": 0.6278,
              "OOD Robustness": 0.6918,
              "Calibration Error": 0.0016,
              "Class Balance": 0.7919,
              "Object Focus": 0.93,
              "Shape Bias": 0.2127,
              Parameters: 217.3,
          },
          
  
          
          {
              Model: "BiTM-ResNet101x1",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ† Supervised learning",
              Text: "‚Ä¢",
              Title: "Big Transfer (BiT): General Visual Representation Learning",
              Accuracy: 0.8234,
              "Adversarial Robustness": 0.0196,
              "Corruption Robustness": 0.6113,
              "OOD Robustness": 0.7078,
              "Calibration Error": 0.0011,
              "Class Balance": 0.7857,
              "Object Focus": 0.9355,
              "Shape Bias": 0.2252,
              Parameters: 44.5,
          },
          
  
          
          {
              Model: "BiTM-ResNet152x2",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ† Supervised learning",
              Text: "‚Ä¢",
              Title: "Big Transfer (BiT): General Visual Representation Learning",
              Accuracy: 0.8451,
              "Adversarial Robustness": 0.039,
              "Corruption Robustness": 0.674,
              "OOD Robustness": 0.716,
              "Calibration Error": 0.0016,
              "Class Balance": 0.7953,
              "Object Focus": 0.9477,
              "Shape Bias": 0.2843,
              Parameters: 236.3,
          },
          
  
          
          {
              Model: "RegNet-y-400mf",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Designing Network Design Spaces",
              Accuracy: 0.7403,
              "Adversarial Robustness": 0.0247,
              "Corruption Robustness": 0.4798,
              "OOD Robustness": 0.5413,
              "Calibration Error": 0.0018,
              "Class Balance": 0.7515,
              "Object Focus": 0.8646,
              "Shape Bias": 0.2218,
              Parameters: 4.3,
          },
          
  
          
          {
              Model: "RegNet-y-800mf",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Designing Network Design Spaces",
              Accuracy: 0.7641,
              "Adversarial Robustness": 0.0297,
              "Corruption Robustness": 0.5232,
              "OOD Robustness": 0.5151,
              "Calibration Error": 0.0017,
              "Class Balance": 0.7582,
              "Object Focus": 0.8869,
              "Shape Bias": 0.2332,
              Parameters: 6.4,
          },
          
  
          
          {
              Model: "RegNet-y-1-6gf",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Designing Network Design Spaces",
              Accuracy: 0.7795,
              "Adversarial Robustness": 0.0301,
              "Corruption Robustness": 0.5446,
              "OOD Robustness": 0.5374,
              "Calibration Error": 0.0018,
              "Class Balance": 0.7632,
              "Object Focus": 0.9062,
              "Shape Bias": 0.2675,
              Parameters: 11.2,
          },
          
  
          
          {
              Model: "RegNet-y-3-2gf",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Designing Network Design Spaces",
              Accuracy: 0.7893,
              "Adversarial Robustness": 0.0297,
              "Corruption Robustness": 0.5526,
              "OOD Robustness": 0.5213,
              "Calibration Error": 0.0019,
              "Class Balance": 0.7663,
              "Object Focus": 0.9099,
              "Shape Bias": 0.2725,
              Parameters: 19.4,
          },
          
  
          
          {
              Model: "RegNet-y-8gf",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Designing Network Design Spaces",
              Accuracy: 0.8002,
              "Adversarial Robustness": 0.0301,
              "Corruption Robustness": 0.5665,
              "OOD Robustness": 0.5312,
              "Calibration Error": 0.0023,
              "Class Balance": 0.7688,
              "Object Focus": 0.9133,
              "Shape Bias": 0.2402,
              Parameters: 39.4,
          },
          
  
          
          {
              Model: "RegNet-y-16gf",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Designing Network Design Spaces",
              Accuracy: 0.8043,
              "Adversarial Robustness": 0.0323,
              "Corruption Robustness": 0.5857,
              "OOD Robustness": 0.5745,
              "Calibration Error": 0.0026,
              "Class Balance": 0.7683,
              "Object Focus": 0.9202,
              "Shape Bias": 0.2662,
              Parameters: 83.6,
          },
          
  
          
          {
              Model: "RegNet-y-32gf",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Designing Network Design Spaces",
              Accuracy: 0.8087,
              "Adversarial Robustness": 0.0484,
              "Corruption Robustness": 0.5979,
              "OOD Robustness": 0.5433,
              "Calibration Error": 0.0026,
              "Class Balance": 0.7694,
              "Object Focus": 0.9151,
              "Shape Bias": 0.2744,
              Parameters: 145.0,
          },
          
  
          
          {
              Model: "VIT-b/16",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
              Accuracy: 0.8107,
              "Adversarial Robustness": 0.1759,
              "Corruption Robustness": 0.6636,
              "OOD Robustness": 0.5561,
              "Calibration Error": 0.0034,
              "Class Balance": 0.7913,
              "Object Focus": 0.9348,
              "Shape Bias": 0.398,
              Parameters: 86.6,
          },
          
  
          
          {
              Model: "VIT-l/16",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
              Accuracy: 0.7966,
              "Adversarial Robustness": 0.2431,
              "Corruption Robustness": 0.5423,
              "OOD Robustness": 0.5039,
              "Calibration Error": 0.0032,
              "Class Balance": 0.7801,
              "Object Focus": 0.9166,
              "Shape Bias": 0.3458,
              Parameters: 304.3,
          },
          
  
          
          {
              Model: "VIT-b/32",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
              Accuracy: 0.7592,
              "Adversarial Robustness": 0.1629,
              "Corruption Robustness": 0.5842,
              "OOD Robustness": 0.5821,
              "Calibration Error": 0.0038,
              "Class Balance": 0.7702,
              "Object Focus": 0.905,
              "Shape Bias": 0.6162,
              Parameters: 88.2,
          },
          
  
          
          {
              Model: "VIT-l/32",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
              Accuracy: 0.7696,
              "Adversarial Robustness": 0.2109,
              "Corruption Robustness": 0.5773,
              "OOD Robustness": 0.6453,
              "Calibration Error": 0.003,
              "Class Balance": 0.7729,
              "Object Focus": 0.9195,
              "Shape Bias": 0.574,
              Parameters: 306.5,
          },
          
  
          
          {
              Model: "Swin-t",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
              Accuracy: 0.8147,
              "Adversarial Robustness": 0.0474,
              "Corruption Robustness": 0.6644,
              "OOD Robustness": 0.5865,
              "Calibration Error": 0.0037,
              "Class Balance": 0.7901,
              "Object Focus": 0.9242,
              "Shape Bias": 0.2191,
              Parameters: 28.3,
          },
          
  
          
          {
              Model: "Swin-s",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
              Accuracy: 0.8319,
              "Adversarial Robustness": 0.081,
              "Corruption Robustness": 0.698,
              "OOD Robustness": 0.5668,
              "Calibration Error": 0.0025,
              "Class Balance": 0.7965,
              "Object Focus": 0.9282,
              "Shape Bias": 0.2018,
              Parameters: 49.6,
          },
          
  
          
          {
              Model: "Swin-b",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
              Accuracy: 0.8358,
              "Adversarial Robustness": 0.1492,
              "Corruption Robustness": 0.7058,
              "OOD Robustness": 0.6004,
              "Calibration Error": 0.003,
              "Class Balance": 0.8014,
              "Object Focus": 0.9276,
              "Shape Bias": 0.2328,
              Parameters: 87.8,
          },
          
  
          
          {
              Model: "EfficientNet-v2-S",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "EfficientNetV2: Smaller Models and Faster Training",
              Accuracy: 0.8316,
              "Adversarial Robustness": 0.0885,
              "Corruption Robustness": 0.6622,
              "OOD Robustness": 0.3165,
              "Calibration Error": 0.0027,
              "Class Balance": 0.7983,
              "Object Focus": 0.9166,
              "Shape Bias": 0.2399,
              Parameters: 21.5,
          },
          
  
          
          {
              Model: "EfficientNet-v2-S",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ† Supervised learning",
              Text: "‚Ä¢",
              Title: "EfficientNetV2: Smaller Models and Faster Training",
              Accuracy: 0.8333,
              "Adversarial Robustness": 0.0602,
              "Corruption Robustness": 0.6781,
              "OOD Robustness": 0.1309,
              "Calibration Error": 0.004,
              "Class Balance": 0.7998,
              "Object Focus": 0.9348,
              "Shape Bias": 0.3403,
              Parameters: 21.5,
          },
          
  
          
          {
              Model: "EfficientNet-v2-M",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "EfficientNetV2: Smaller Models and Faster Training",
              Accuracy: 0.8456,
              "Adversarial Robustness": 0.1532,
              "Corruption Robustness": 0.7072,
              "OOD Robustness": 0.5282,
              "Calibration Error": 0.0056,
              "Class Balance": 0.8159,
              "Object Focus": 0.9466,
              "Shape Bias": 0.2515,
              Parameters: 54.1,
          },
          
  
          
          {
              Model: "EfficientNet-v2-M",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ† Supervised learning",
              Text: "‚Ä¢",
              Title: "EfficientNetV2: Smaller Models and Faster Training",
              Accuracy: 0.8477,
              "Adversarial Robustness": 0.153,
              "Corruption Robustness": 0.7112,
              "OOD Robustness": 0.1606,
              "Calibration Error": 0.0043,
              "Class Balance": 0.8051,
              "Object Focus": 0.9508,
              "Shape Bias": 0.3646,
              Parameters: 54.1,
          },
          
  
          
          {
              Model: "EfficientNet-v2-L",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "EfficientNetV2: Smaller Models and Faster Training",
              Accuracy: 0.852,
              "Adversarial Robustness": 0.1725,
              "Corruption Robustness": 0.7391,
              "OOD Robustness": 0.6403,
              "Calibration Error": 0.0052,
              "Class Balance": 0.817,
              "Object Focus": 0.9357,
              "Shape Bias": 0.2658,
              Parameters: 118.5,
          },
          
  
          
          {
              Model: "EfficientNet-v2-L",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ† Supervised learning",
              Text: "‚Ä¢",
              Title: "EfficientNetV2: Smaller Models and Faster Training",
              Accuracy: 0.8582,
              "Adversarial Robustness": 0.1689,
              "Corruption Robustness": 0.7269,
              "OOD Robustness": 0.1289,
              "Calibration Error": 0.0046,
              "Class Balance": 0.8119,
              "Object Focus": 0.9606,
              "Shape Bias": 0.3826,
              Parameters: 118.5,
          },
          
  
          
          {
              Model: "DeiT-t",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Training data-efficient image transformers & distillation through attention",
              Accuracy: 0.7216,
              "Adversarial Robustness": 0.1342,
              "Corruption Robustness": 0.6064,
              "OOD Robustness": 0.5493,
              "Calibration Error": 0.0053,
              "Class Balance": 0.7708,
              "Object Focus": 0.9011,
              "Shape Bias": 0.2435,
              Parameters: 5.7,
          },
          
  
          
          {
              Model: "DeiT-s",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Training data-efficient image transformers & distillation through attention",
              Accuracy: 0.7986,
              "Adversarial Robustness": 0.222,
              "Corruption Robustness": 0.705,
              "OOD Robustness": 0.588,
              "Calibration Error": 0.0043,
              "Class Balance": 0.7893,
              "Object Focus": 0.9416,
              "Shape Bias": 0.3389,
              Parameters: 22.1,
          },
          
  
          
          {
              Model: "DeiT-b",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Training data-efficient image transformers & distillation through attention",
              Accuracy: 0.8198,
              "Adversarial Robustness": 0.2326,
              "Corruption Robustness": 0.746,
              "OOD Robustness": 0.6245,
              "Calibration Error": 0.0038,
              "Class Balance": 0.7956,
              "Object Focus": 0.9464,
              "Shape Bias": 0.3941,
              Parameters: 86.6,
          },
          
  
          
          {
              Model: "ConViT-t",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases",
              Accuracy: 0.7311,
              "Adversarial Robustness": 0.1492,
              "Corruption Robustness": 0.6202,
              "OOD Robustness": 0.5811,
              "Calibration Error": 0.0051,
              "Class Balance": 0.7724,
              "Object Focus": 0.9092,
              "Shape Bias": 0.2669,
              Parameters: 5.7,
          },
          
  
          
          {
              Model: "ConViT-s",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases",
              Accuracy: 0.814,
              "Adversarial Robustness": 0.2459,
              "Corruption Robustness": 0.7302,
              "OOD Robustness": 0.6099,
              "Calibration Error": 0.0046,
              "Class Balance": 0.7983,
              "Object Focus": 0.9442,
              "Shape Bias": 0.3418,
              Parameters: 27.8,
          },
          
  
          
          {
              Model: "ConViT-b",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases",
              Accuracy: 0.8229,
              "Adversarial Robustness": 0.2687,
              "Corruption Robustness": 0.7521,
              "OOD Robustness": 0.7051,
              "Calibration Error": 0.0043,
              "Class Balance": 0.7993,
              "Object Focus": 0.9517,
              "Shape Bias": 0.3899,
              Parameters: 86.5,
          },
          
  
          
          {
              Model: "CaiT-xxs24",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Going deeper with Image Transformers",
              Accuracy: 0.8095,
              "Adversarial Robustness": 0.165,
              "Corruption Robustness": 0.7029,
              "OOD Robustness": 0.571,
              "Calibration Error": 0.0022,
              "Class Balance": 0.7844,
              "Object Focus": 0.9372,
              "Shape Bias": 0.2286,
              Parameters: 12.0,
          },
          
  
          
          {
              Model: "CaiT-xs24",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Going deeper with Image Transformers",
              Accuracy: 0.8405,
              "Adversarial Robustness": 0.2632,
              "Corruption Robustness": 0.7457,
              "OOD Robustness": 0.6527,
              "Calibration Error": 0.0022,
              "Class Balance": 0.7993,
              "Object Focus": 0.9465,
              "Shape Bias": 0.2195,
              Parameters: 26.7,
          },
          
  
          
          {
              Model: "CaiT-s24",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Going deeper with Image Transformers",
              Accuracy: 0.8504,
              "Adversarial Robustness": 0.3026,
              "Corruption Robustness": 0.766,
              "OOD Robustness": 0.6399,
              "Calibration Error": 0.0022,
              "Class Balance": 0.8043,
              "Object Focus": 0.9547,
              "Shape Bias": 0.2899,
              Parameters: 47.1,
          },
          
  
          
          {
              Model: "CrossViT-9‚Ä†",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification",
              Accuracy: 0.7699,
              "Adversarial Robustness": 0.1538,
              "Corruption Robustness": 0.6394,
              "OOD Robustness": 0.5638,
              "Calibration Error": 0.0041,
              "Class Balance": 0.7778,
              "Object Focus": 0.9282,
              "Shape Bias": 0.2622,
              Parameters: 8.8,
          },
          
  
          
          {
              Model: "CrossViT-15‚Ä†",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification",
              Accuracy: 0.8231,
              "Adversarial Robustness": 0.249,
              "Corruption Robustness": 0.7256,
              "OOD Robustness": 0.6167,
              "Calibration Error": 0.0037,
              "Class Balance": 0.7987,
              "Object Focus": 0.943,
              "Shape Bias": 0.3554,
              Parameters: 28.2,
          },
          
  
          
          {
              Model: "CrossViT-18‚Ä†",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification",
              Accuracy: 0.825,
              "Adversarial Robustness": 0.2554,
              "Corruption Robustness": 0.7397,
              "OOD Robustness": 0.6162,
              "Calibration Error": 0.0036,
              "Class Balance": 0.7992,
              "Object Focus": 0.9392,
              "Shape Bias": 0.3633,
              Parameters: 44.3,
          },
          
  
          
          {
              Model: "XCiT-s24-16 ",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "XCiT: Cross-Covariance Image Transformers",
              Accuracy: 0.8258,
              "Adversarial Robustness": 0.1989,
              "Corruption Robustness": 0.7392,
              "OOD Robustness": 0.6705,
              "Calibration Error": 0.004,
              "Class Balance": 0.801,
              "Object Focus": 0.9276,
              "Shape Bias": 0.3216,
              Parameters: 47.7,
          },
          
  
          
          {
              Model: "XCiT-m24-16",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "XCiT: Cross-Covariance Image Transformers",
              Accuracy: 0.8263,
              "Adversarial Robustness": 0.2083,
              "Corruption Robustness": 0.7419,
              "OOD Robustness": 0.6772,
              "Calibration Error": 0.0038,
              "Class Balance": 0.7987,
              "Object Focus": 0.9298,
              "Shape Bias": 0.3276,
              Parameters: 84.4,
          },
          
  
          
          {
              Model: "XCiT-l24-16 ",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "XCiT: Cross-Covariance Image Transformers",
              Accuracy: 0.829,
              "Adversarial Robustness": 0.2224,
              "Corruption Robustness": 0.7391,
              "OOD Robustness": 0.7215,
              "Calibration Error": 0.0037,
              "Class Balance": 0.7987,
              "Object Focus": 0.9339,
              "Shape Bias": 0.3375,
              Parameters: 189.1,
          },
          
  
          
          {
              Model: "LeViT-128",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "LeViT: A Vision Transformer in {ConvNet}'s Clothing for Faster Inference",
              Accuracy: 0.7847,
              "Adversarial Robustness": 0.169,
              "Corruption Robustness": 0.6357,
              "OOD Robustness": 0.5898,
              "Calibration Error": 0.0017,
              "Class Balance": 0.7645,
              "Object Focus": 0.9185,
              "Shape Bias": 0.2632,
              Parameters: 9.2,
          },
          
  
          
          {
              Model: "LeViT-256",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "LeViT: A Vision Transformer in {ConvNet}'s Clothing for Faster Inference",
              Accuracy: 0.8152,
              "Adversarial Robustness": 0.1793,
              "Corruption Robustness": 0.6599,
              "OOD Robustness": 0.5802,
              "Calibration Error": 0.0022,
              "Class Balance": 0.7743,
              "Object Focus": 0.9306,
              "Shape Bias": 0.2297,
              Parameters: 18.9,
          },
          
  
          
          {
              Model: "LeViT-384",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "LeViT: A Vision Transformer in {ConvNet}'s Clothing for Faster Inference",
              Accuracy: 0.8259,
              "Adversarial Robustness": 0.1867,
              "Corruption Robustness": 0.6735,
              "OOD Robustness": 0.6345,
              "Calibration Error": 0.0023,
              "Class Balance": 0.7805,
              "Object Focus": 0.9363,
              "Shape Bias": 0.2401,
              Parameters: 39.1,
          },
          
  
          
          {
              Model: "PiT-t",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Rethinking Spatial Dimensions of Vision Transformers",
              Accuracy: 0.7293,
              "Adversarial Robustness": 0.1337,
              "Corruption Robustness": 0.6046,
              "OOD Robustness": 0.5907,
              "Calibration Error": 0.0051,
              "Class Balance": 0.7711,
              "Object Focus": 0.8961,
              "Shape Bias": 0.3017,
              Parameters: 4.8,
          },
          
  
          
          {
              Model: "PiT-xs",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Rethinking Spatial Dimensions of Vision Transformers",
              Accuracy: 0.7817,
              "Adversarial Robustness": 0.1815,
              "Corruption Robustness": 0.6556,
              "OOD Robustness": 0.5849,
              "Calibration Error": 0.0046,
              "Class Balance": 0.7846,
              "Object Focus": 0.9248,
              "Shape Bias": 0.3108,
              Parameters: 10.6,
          },
          
  
          
          {
              Model: "PiT-s",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Rethinking Spatial Dimensions of Vision Transformers",
              Accuracy: 0.8108,
              "Adversarial Robustness": 0.2258,
              "Corruption Robustness": 0.7029,
              "OOD Robustness": 0.6077,
              "Calibration Error": 0.0042,
              "Class Balance": 0.7945,
              "Object Focus": 0.9309,
              "Shape Bias": 0.3377,
              Parameters: 23.5,
          },
          
  
          
          {
              Model: "PiT-b",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Rethinking Spatial Dimensions of Vision Transformers",
              Accuracy: 0.8245,
              "Adversarial Robustness": 0.2811,
              "Corruption Robustness": 0.7357,
              "OOD Robustness": 0.6179,
              "Calibration Error": 0.0037,
              "Class Balance": 0.7954,
              "Object Focus": 0.9443,
              "Shape Bias": 0.3338,
              Parameters: 73.8,
          },
          
  
          
          {
              Model: "CoaT-t-lite",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Co-Scale Conv-attentional Image Transformers",
              Accuracy: 0.7751,
              "Adversarial Robustness": 0.1237,
              "Corruption Robustness": 0.5971,
              "OOD Robustness": 0.5636,
              "Calibration Error": 0.0053,
              "Class Balance": 0.785,
              "Object Focus": 0.9267,
              "Shape Bias": 0.2539,
              Parameters: 5.7,
          },
          
  
          
          {
              Model: "CoaT-mi-lite",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Co-Scale Conv-attentional Image Transformers",
              Accuracy: 0.791,
              "Adversarial Robustness": 0.1492,
              "Corruption Robustness": 0.6298,
              "OOD Robustness": 0.5717,
              "Calibration Error": 0.0051,
              "Class Balance": 0.7891,
              "Object Focus": 0.9348,
              "Shape Bias": 0.2714,
              Parameters: 11.0,
          },
          
  
          
          {
              Model: "CoaT-s-lite",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Co-Scale Conv-attentional Image Transformers",
              Accuracy: 0.823,
              "Adversarial Robustness": 0.298,
              "Corruption Robustness": 0.7046,
              "OOD Robustness": 0.586,
              "Calibration Error": 0.0038,
              "Class Balance": 0.799,
              "Object Focus": 0.9325,
              "Shape Bias": 0.2995,
              Parameters: 19.8,
          },
          
  
          
          {
              Model: "CoaT-me-lite",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Co-Scale Conv-attentional Image Transformers",
              Accuracy: 0.836,
              "Adversarial Robustness": 0.2188,
              "Corruption Robustness": 0.7513,
              "OOD Robustness": 0.6595,
              "Calibration Error": 0.0037,
              "Class Balance": 0.8057,
              "Object Focus": 0.9519,
              "Shape Bias": 0.3121,
              Parameters: 44.6,
          },
          
  
          
          {
              Model: "MaxViT-t",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "MaxViT: Multi-axis Vision Transformer",
              Accuracy: 0.837,
              "Adversarial Robustness": 0.2561,
              "Corruption Robustness": 0.7449,
              "OOD Robustness": 0.6561,
              "Calibration Error": 0.0037,
              "Class Balance": 0.8071,
              "Object Focus": 0.9304,
              "Shape Bias": 0.275,
              Parameters: 30.9,
          },
          
  
          
          {
              Model: "MaxViT-b",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "MaxViT: Multi-axis Vision Transformer",
              Accuracy: 0.8488,
              "Adversarial Robustness": 0.4218,
              "Corruption Robustness": 0.7703,
              "OOD Robustness": 0.7199,
              "Calibration Error": 0.0045,
              "Class Balance": 0.8117,
              "Object Focus": 0.9459,
              "Shape Bias": 0.3511,
              Parameters: 119.5,
          },
          
  
          
          {
              Model: "MaxViT-l",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "MaxViT: Multi-axis Vision Transformer",
              Accuracy: 0.8493,
              "Adversarial Robustness": 0.4088,
              "Corruption Robustness": 0.7707,
              "OOD Robustness": 0.7129,
              "Calibration Error": 0.0053,
              "Class Balance": 0.814,
              "Object Focus": 0.9506,
              "Shape Bias": 0.3497,
              Parameters: 211.8,
          },
          
  
          
          {
              Model: "DeiT3-s",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "DeiT III: Revenge of the ViT",
              Accuracy: 0.8136,
              "Adversarial Robustness": 0.1516,
              "Corruption Robustness": 0.738,
              "OOD Robustness": 0.5885,
              "Calibration Error": 0.0023,
              "Class Balance": 0.7731,
              "Object Focus": 0.9472,
              "Shape Bias": 0.3404,
              Parameters: 22.1,
          },
          
  
          
          {
              Model: "DeiT3-s",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ† Supervised learning",
              Text: "‚Ä¢",
              Title: "DeiT III: Revenge of the ViT",
              Accuracy: 0.8307,
              "Adversarial Robustness": 0.2751,
              "Corruption Robustness": 0.6678,
              "OOD Robustness": 0.7019,
              "Calibration Error": 0.0033,
              "Class Balance": 0.8011,
              "Object Focus": 0.9236,
              "Shape Bias": 0.3502,
              Parameters: 22.1,
          },
          
  
          
          {
              Model: "DeiT3-m",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "DeiT III: Revenge of the ViT",
              Accuracy: 0.8309,
              "Adversarial Robustness": 0.2654,
              "Corruption Robustness": 0.7593,
              "OOD Robustness": 0.6593,
              "Calibration Error": 0.004,
              "Class Balance": 0.7995,
              "Object Focus": 0.9501,
              "Shape Bias": 0.423,
              Parameters: 38.8,
          },
          
  
          
          {
              Model: "DeiT3-m",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ† Supervised learning",
              Text: "‚Ä¢",
              Title: "DeiT III: Revenge of the ViT",
              Accuracy: 0.8456,
              "Adversarial Robustness": 0.2428,
              "Corruption Robustness": 0.6891,
              "OOD Robustness": 0.7067,
              "Calibration Error": 0.0032,
              "Class Balance": 0.807,
              "Object Focus": 0.9267,
              "Shape Bias": 0.3554,
              Parameters: 38.8,
          },
          
  
          
          {
              Model: "DeiT3-b",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "DeiT III: Revenge of the ViT",
              Accuracy: 0.8379,
              "Adversarial Robustness": 0.2665,
              "Corruption Robustness": 0.7871,
              "OOD Robustness": 0.6955,
              "Calibration Error": 0.0036,
              "Class Balance": 0.8013,
              "Object Focus": 0.9543,
              "Shape Bias": 0.4319,
              Parameters: 86.6,
          },
          
  
          
          {
              Model: "DeiT3-b",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ† Supervised learning",
              Text: "‚Ä¢",
              Title: "DeiT III: Revenge of the ViT",
              Accuracy: 0.8571,
              "Adversarial Robustness": 0.2599,
              "Corruption Robustness": 0.7272,
              "OOD Robustness": 0.7603,
              "Calibration Error": 0.0027,
              "Class Balance": 0.8137,
              "Object Focus": 0.9493,
              "Shape Bias": 0.3986,
              Parameters: 86.6,
          },
          
  
          
          {
              Model: "DeiT3-l",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "DeiT III: Revenge of the ViT",
              Accuracy: 0.8478,
              "Adversarial Robustness": 0.3184,
              "Corruption Robustness": 0.8207,
              "OOD Robustness": 0.756,
              "Calibration Error": 0.0031,
              "Class Balance": 0.8055,
              "Object Focus": 0.9586,
              "Shape Bias": 0.5017,
              Parameters: 304.4,
          },
          
  
          
          {
              Model: "DeiT3-l",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ† Supervised learning",
              Text: "‚Ä¢",
              Title: "DeiT III: Revenge of the ViT",
              Accuracy: 0.8698,
              "Adversarial Robustness": 0.3488,
              "Corruption Robustness": 0.7981,
              "OOD Robustness": 0.8675,
              "Calibration Error": 0.0024,
              "Class Balance": 0.8185,
              "Object Focus": 0.9554,
              "Shape Bias": 0.5131,
              Parameters: 304.4,
          },
          
  
          
          {
              Model: "MViTv2-t",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "MViTv2: Improved Multiscale Vision Transformers for Classification and Detection",
              Accuracy: 0.824,
              "Adversarial Robustness": 0.2754,
              "Corruption Robustness": 0.7198,
              "OOD Robustness": 0.7181,
              "Calibration Error": 0.0054,
              "Class Balance": 0.7983,
              "Object Focus": 0.9494,
              "Shape Bias": 0.2923,
              Parameters: 24.2,
          },
          
  
          
          {
              Model: "MViTv2-s",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "MViTv2: Improved Multiscale Vision Transformers for Classification and Detection",
              Accuracy: 0.8377,
              "Adversarial Robustness": 0.3434,
              "Corruption Robustness": 0.7532,
              "OOD Robustness": 0.7509,
              "Calibration Error": 0.0048,
              "Class Balance": 0.8045,
              "Object Focus": 0.9561,
              "Shape Bias": 0.2888,
              Parameters: 34.9,
          },
          
  
          
          {
              Model: "MViTv2-b",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "MViTv2: Improved Multiscale Vision Transformers for Classification and Detection",
              Accuracy: 0.8444,
              "Adversarial Robustness": 0.3368,
              "Corruption Robustness": 0.7592,
              "OOD Robustness": 0.734,
              "Calibration Error": 0.0035,
              "Class Balance": 0.8066,
              "Object Focus": 0.9468,
              "Shape Bias": 0.3318,
              Parameters: 51.5,
          },
          
  
          
          {
              Model: "MViTv2-l",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "MViTv2: Improved Multiscale Vision Transformers for Classification and Detection",
              Accuracy: 0.8525,
              "Adversarial Robustness": 0.4402,
              "Corruption Robustness": 0.7956,
              "OOD Robustness": 0.8227,
              "Calibration Error": 0.0035,
              "Class Balance": 0.8108,
              "Object Focus": 0.9606,
              "Shape Bias": 0.3442,
              Parameters: 218.0,
          },
          
  
          
          {
              Model: "SwinV2-t/8",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Swin Transformer V2: Scaling Up Capacity and Resolution",
              Accuracy: 0.8207,
              "Adversarial Robustness": 0.0446,
              "Corruption Robustness": 0.6581,
              "OOD Robustness": 0.5332,
              "Calibration Error": 0.0044,
              "Class Balance": 0.7967,
              "Object Focus": 0.923,
              "Shape Bias": 0.1843,
              Parameters: 28.4,
          },
          
  
          
          {
              Model: "SwinV2-s/8",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Swin Transformer V2: Scaling Up Capacity and Resolution",
              Accuracy: 0.8371,
              "Adversarial Robustness": 0.0413,
              "Corruption Robustness": 0.7091,
              "OOD Robustness": 0.5891,
              "Calibration Error": 0.0036,
              "Class Balance": 0.8032,
              "Object Focus": 0.9257,
              "Shape Bias": 0.2016,
              Parameters: 49.7,
          },
          
  
          
          {
              Model: "SwinV2-b/8",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Swin Transformer V2: Scaling Up Capacity and Resolution",
              Accuracy: 0.8411,
              "Adversarial Robustness": 0.0413,
              "Corruption Robustness": 0.6988,
              "OOD Robustness": 0.5963,
              "Calibration Error": 0.0033,
              "Class Balance": 0.806,
              "Object Focus": 0.9304,
              "Shape Bias": 0.2035,
              Parameters: 87.9,
          },
          
  
          
          {
              Model: "SwinV2-t/16",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Swin Transformer V2: Scaling Up Capacity and Resolution",
              Accuracy: 0.8283,
              "Adversarial Robustness": 0.184,
              "Corruption Robustness": 0.7036,
              "OOD Robustness": 0.6156,
              "Calibration Error": 0.0043,
              "Class Balance": 0.7976,
              "Object Focus": 0.9383,
              "Shape Bias": 0.2497,
              Parameters: 28.3,
          },
          
  
          
          {
              Model: "SwinV2-s/16",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Swin Transformer V2: Scaling Up Capacity and Resolution",
              Accuracy: 0.8423,
              "Adversarial Robustness": 0.2605,
              "Corruption Robustness": 0.7463,
              "OOD Robustness": 0.6235,
              "Calibration Error": 0.0033,
              "Class Balance": 0.8043,
              "Object Focus": 0.9411,
              "Shape Bias": 0.303,
              Parameters: 49.7,
          },
          
  
          
          {
              Model: "SwinV2-b/16",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Swin Transformer V2: Scaling Up Capacity and Resolution",
              Accuracy: 0.846,
              "Adversarial Robustness": 0.2921,
              "Corruption Robustness": 0.7573,
              "OOD Robustness": 0.6568,
              "Calibration Error": 0.0032,
              "Class Balance": 0.806,
              "Object Focus": 0.9454,
              "Shape Bias": 0.2956,
              Parameters: 87.9,
          },
          
  
          
          {
              Model: "SwinV2-b/12to16",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ† Supervised learning",
              Text: "‚Ä¢",
              Title: "Swin Transformer V2: Scaling Up Capacity and Resolution",
              Accuracy: 0.8627,
              "Adversarial Robustness": 0.2647,
              "Corruption Robustness": 0.813,
              "OOD Robustness": 0.8117,
              "Calibration Error": 0.004,
              "Class Balance": 0.8184,
              "Object Focus": 0.9602,
              "Shape Bias": 0.4077,
              Parameters: 87.9,
          },
          
  
          
          {
              Model: "SwinV2-l/12to16",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ† Supervised learning",
              Text: "‚Ä¢",
              Title: "Swin Transformer V2: Scaling Up Capacity and Resolution",
              Accuracy: 0.8694,
              "Adversarial Robustness": 0.2887,
              "Corruption Robustness": 0.8241,
              "OOD Robustness": 0.8011,
              "Calibration Error": 0.0038,
              "Class Balance": 0.8208,
              "Object Focus": 0.9679,
              "Shape Bias": 0.4152,
              Parameters: 196.7,
          },
          
  
          
          {
              Model: "TinyViT-5M/16",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
              Accuracy: 0.7917,
              "Adversarial Robustness": 0.1479,
              "Corruption Robustness": 0.6404,
              "OOD Robustness": 0.5831,
              "Calibration Error": 0.011,
              "Class Balance": 0.7966,
              "Object Focus": 0.927,
              "Shape Bias": 0.2436,
              Parameters: 5.4,
          },
          
  
          
          {
              Model: "TinyViT-5M/16",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ† Supervised learning",
              Text: "‚Ä¢",
              Title: "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
              Accuracy: 0.8088,
              "Adversarial Robustness": 0.0854,
              "Corruption Robustness": 0.6733,
              "OOD Robustness": 0.6559,
              "Calibration Error": 0.0041,
              "Class Balance": 0.7861,
              "Object Focus": 0.9206,
              "Shape Bias": 0.3253,
              Parameters: 5.4,
          },
          
  
          
          {
              Model: "TinyViT-11M/16",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
              Accuracy: 0.815,
              "Adversarial Robustness": 0.2088,
              "Corruption Robustness": 0.694,
              "OOD Robustness": 0.6279,
              "Calibration Error": 0.0096,
              "Class Balance": 0.808,
              "Object Focus": 0.9361,
              "Shape Bias": 0.2604,
              Parameters: 11.0,
          },
          
  
          
          {
              Model: "TinyViT-11M/16",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ† Supervised learning",
              Text: "‚Ä¢",
              Title: "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
              Accuracy: 0.8324,
              "Adversarial Robustness": 0.1138,
              "Corruption Robustness": 0.704,
              "OOD Robustness": 0.7345,
              "Calibration Error": 0.0039,
              "Class Balance": 0.7975,
              "Object Focus": 0.9478,
              "Shape Bias": 0.3329,
              Parameters: 11.0,
          },
          
  
          
          {
              Model: "TinyViT-21M/16",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
              Accuracy: 0.8325,
              "Adversarial Robustness": 0.2361,
              "Corruption Robustness": 0.731,
              "OOD Robustness": 0.6615,
              "Calibration Error": 0.0077,
              "Class Balance": 0.8127,
              "Object Focus": 0.9463,
              "Shape Bias": 0.3164,
              Parameters: 21.2,
          },
          
  
          
          {
              Model: "TinyViT-21M/16",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ† Supervised learning",
              Text: "‚Ä¢",
              Title: "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
              Accuracy: 0.8508,
              "Adversarial Robustness": 0.1286,
              "Corruption Robustness": 0.7535,
              "OOD Robustness": 0.8012,
              "Calibration Error": 0.0034,
              "Class Balance": 0.807,
              "Object Focus": 0.9515,
              "Shape Bias": 0.3698,
              Parameters: 21.2,
          },
          
  
          
          {
              Model: "ViT-s/16",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "How to train your ViT? {D}ata, Augmentation, and Regularization in Vision Transformers",
              Accuracy: 0.7884,
              "Adversarial Robustness": 0.074,
              "Corruption Robustness": 0.6732,
              "OOD Robustness": 0.5551,
              "Calibration Error": 0.0018,
              "Class Balance": 0.765,
              "Object Focus": 0.912,
              "Shape Bias": 0.2988,
              Parameters: 22.1,
          },
          
  
          
          {
              Model: "ViT-s/16",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ† Supervised learning",
              Text: "‚Ä¢",
              Title: "How to train your ViT? {D}ata, Augmentation, and Regularization in Vision Transformers",
              Accuracy: 0.8138,
              "Adversarial Robustness": 0.0281,
              "Corruption Robustness": 0.7127,
              "OOD Robustness": 0.4498,
              "Calibration Error": 0.001,
              "Class Balance": 0.7826,
              "Object Focus": 0.9372,
              "Shape Bias": 0.3819,
              Parameters: 22.1,
          },
          
  
          
          {
              Model: "ViT-b/16",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ† Supervised learning",
              Text: "‚Ä¢",
              Title: "How to train your ViT? {D}ata, Augmentation, and Regularization in Vision Transformers",
              Accuracy: 0.8454,
              "Adversarial Robustness": 0.1,
              "Corruption Robustness": 0.7866,
              "OOD Robustness": 0.3888,
              "Calibration Error": 0.0009,
              "Class Balance": 0.798,
              "Object Focus": 0.9576,
              "Shape Bias": 0.4652,
              Parameters: 86.6,
          },
          
  
          
          {
              Model: "ViT-l/16",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ† Supervised learning",
              Text: "‚Ä¢",
              Title: "How to train your ViT? {D}ata, Augmentation, and Regularization in Vision Transformers",
              Accuracy: 0.8072,
              "Adversarial Robustness": 0.0934,
              "Corruption Robustness": 0.7491,
              "OOD Robustness": 0.4673,
              "Calibration Error": 0.0011,
              "Class Balance": 0.7786,
              "Object Focus": 0.9251,
              "Shape Bias": 0.5329,
              Parameters: 88.2,
          },
          
  
          
          {
              Model: "ViT-b/32",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ† Supervised learning",
              Text: "‚Ä¢",
              Title: "How to train your ViT? {D}ata, Augmentation, and Regularization in Vision Transformers",
              Accuracy: 0.8584,
              "Adversarial Robustness": 0.2365,
              "Corruption Robustness": 0.8409,
              "OOD Robustness": 0.7655,
              "Calibration Error": 0.0011,
              "Class Balance": 0.8051,
              "Object Focus": 0.9638,
              "Shape Bias": 0.5442,
              Parameters: 304.3,
          },
          
  
          
          {
              Model: "ViT-l/32",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ† Supervised learning",
              Text: "‚Ä¢",
              Title: "How to train your ViT? {D}ata, Augmentation, and Regularization in Vision Transformers",
              Accuracy: 0.8151,
              "Adversarial Robustness": 0.0854,
              "Corruption Robustness": 0.7464,
              "OOD Robustness": 0.6264,
              "Calibration Error": 0.0022,
              "Class Balance": 0.7734,
              "Object Focus": 0.9209,
              "Shape Bias": 0.4746,
              Parameters: 306.6,
          },
          
  
          
          {
              Model: "ConvNext-T",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "A ConvNet for the 2020s",
              Accuracy: 0.8252,
              "Adversarial Robustness": 0.0933,
              "Corruption Robustness": 0.6457,
              "OOD Robustness": 0.6002,
              "Calibration Error": 0.0077,
              "Class Balance": 0.8054,
              "Object Focus": 0.9246,
              "Shape Bias": 0.2514,
              Parameters: 28.6,
          },
          
  
          
          {
              Model: "ConvNext-T",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ† Supervised learning",
              Text: "‚Ä¢",
              Title: "A ConvNet for the 2020s",
              Accuracy: 0.829,
              "Adversarial Robustness": 0.1581,
              "Corruption Robustness": 0.71,
              "OOD Robustness": 0.6922,
              "Calibration Error": 0.0037,
              "Class Balance": 0.7992,
              "Object Focus": 0.9482,
              "Shape Bias": 0.2865,
              Parameters: 28.6,
          },
          
  
          
          {
              Model: "ConvNext-S",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "A ConvNet for the 2020s",
              Accuracy: 0.8363,
              "Adversarial Robustness": 0.1115,
              "Corruption Robustness": 0.6822,
              "OOD Robustness": 0.6509,
              "Calibration Error": 0.0083,
              "Class Balance": 0.8147,
              "Object Focus": 0.938,
              "Shape Bias": 0.2536,
              Parameters: 50.2,
          },
          
  
          
          {
              Model: "ConvNext-S",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ† Supervised learning",
              Text: "‚Ä¢",
              Title: "A ConvNet for the 2020s",
              Accuracy: 0.8459,
              "Adversarial Robustness": 0.201,
              "Corruption Robustness": 0.7581,
              "OOD Robustness": 0.7567,
              "Calibration Error": 0.0022,
              "Class Balance": 0.8038,
              "Object Focus": 0.9632,
              "Shape Bias": 0.3115,
              Parameters: 50.2,
          },
          
  
          
          {
              Model: "ConvNext-B",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "A ConvNet for the 2020s",
              Accuracy: 0.8406,
              "Adversarial Robustness": 0.1159,
              "Corruption Robustness": 0.7006,
              "OOD Robustness": 0.6634,
              "Calibration Error": 0.0089,
              "Class Balance": 0.8192,
              "Object Focus": 0.9498,
              "Shape Bias": 0.2982,
              Parameters: 88.6,
          },
          
  
          
          {
              Model: "ConvNext-B",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ† Supervised learning",
              Text: "‚Ä¢",
              Title: "A ConvNet for the 2020s",
              Accuracy: 0.8581,
              "Adversarial Robustness": 0.205,
              "Corruption Robustness": 0.7816,
              "OOD Robustness": 0.8178,
              "Calibration Error": 0.0019,
              "Class Balance": 0.8093,
              "Object Focus": 0.9635,
              "Shape Bias": 0.3348,
              Parameters: 88.6,
          },
          
  
          
          {
              Model: "ConvNext-L",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "A ConvNet for the 2020s",
              Accuracy: 0.8441,
              "Adversarial Robustness": 0.1222,
              "Corruption Robustness": 0.7193,
              "OOD Robustness": 0.6662,
              "Calibration Error": 0.0064,
              "Class Balance": 0.8135,
              "Object Focus": 0.9473,
              "Shape Bias": 0.3113,
              Parameters: 197.8,
          },
          
  
          
          {
              Model: "ConvNext-L",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ† Supervised learning",
              Text: "‚Ä¢",
              Title: "A ConvNet for the 2020s",
              Accuracy: 0.8661,
              "Adversarial Robustness": 0.2236,
              "Corruption Robustness": 0.8066,
              "OOD Robustness": 0.8274,
              "Calibration Error": 0.0016,
              "Class Balance": 0.8126,
              "Object Focus": 0.9695,
              "Shape Bias": 0.4009,
              Parameters: 197.8,
          },
          
  
          
          {
              Model: "BeiT-b",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚Ä¢",
              Title: "BEiT: BERT Pre-Training of Image Transformers",
              Accuracy: 0.8522,
              "Adversarial Robustness": 0.0744,
              "Corruption Robustness": 0.7924,
              "OOD Robustness": 0.8009,
              "Calibration Error": 0.0036,
              "Class Balance": 0.8093,
              "Object Focus": 0.9609,
              "Shape Bias": 0.5293,
              Parameters: 86.5,
          },
          
  
          
          {
              Model: "EfficientFormer-l1",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "EfficientFormer: Vision Transformers at {MobileNet} Speed}",
              Accuracy: 0.805,
              "Adversarial Robustness": 0.1402,
              "Corruption Robustness": 0.632,
              "OOD Robustness": 0.5746,
              "Calibration Error": 0.0014,
              "Class Balance": 0.772,
              "Object Focus": 0.9393,
              "Shape Bias": 0.2057,
              Parameters: 12.3,
          },
          
  
          
          {
              Model: "EfficientFormer-l3",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "EfficientFormer: Vision Transformers at {MobileNet} Speed}",
              Accuracy: 0.8256,
              "Adversarial Robustness": 0.2016,
              "Corruption Robustness": 0.6883,
              "OOD Robustness": 0.6504,
              "Calibration Error": 0.002,
              "Class Balance": 0.7811,
              "Object Focus": 0.9369,
              "Shape Bias": 0.2405,
              Parameters: 31.4,
          },
          
  
          
          {
              Model: "EfficientFormer-l7",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "EfficientFormer: Vision Transformers at {MobileNet} Speed}",
              Accuracy: 0.8337,
              "Adversarial Robustness": 0.2234,
              "Corruption Robustness": 0.6978,
              "OOD Robustness": 0.6812,
              "Calibration Error": 0.0023,
              "Class Balance": 0.7845,
              "Object Focus": 0.9449,
              "Shape Bias": 0.229,
              Parameters: 82.2,
          },
          
  
          
          {
              Model: "DaViT-t",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "DaViT: Dual Attention Vision Transformers",
              Accuracy: 0.827,
              "Adversarial Robustness": 0.187,
              "Corruption Robustness": 0.7188,
              "OOD Robustness": 0.6232,
              "Calibration Error": 0.0038,
              "Class Balance": 0.7976,
              "Object Focus": 0.9409,
              "Shape Bias": 0.3394,
              Parameters: 28.4,
          },
          
  
          
          {
              Model: "DaViT-s",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "DaViT: Dual Attention Vision Transformers",
              Accuracy: 0.8425,
              "Adversarial Robustness": 0.2355,
              "Corruption Robustness": 0.7513,
              "OOD Robustness": 0.7025,
              "Calibration Error": 0.0022,
              "Class Balance": 0.8012,
              "Object Focus": 0.9481,
              "Shape Bias": 0.3486,
              Parameters: 49.7,
          },
          
  
          
          {
              Model: "DaViT-b",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "DaViT: Dual Attention Vision Transformers",
              Accuracy: 0.8465,
              "Adversarial Robustness": 0.2985,
              "Corruption Robustness": 0.7537,
              "OOD Robustness": 0.6792,
              "Calibration Error": 0.002,
              "Class Balance": 0.7998,
              "Object Focus": 0.9446,
              "Shape Bias": 0.3457,
              Parameters: 88.0,
          },
          
  
          
          {
              Model: "Swin-B",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚óè Adversarial training",
              Text: "",
              Title: "A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking",
              Accuracy: 0.7679,
              "Adversarial Robustness": 0.588,
              "Corruption Robustness": 0.651,
              "OOD Robustness": 0.7418,
              "Calibration Error": 0.0083,
              "Class Balance": 0.786,
              "Object Focus": 0.9408,
              "Shape Bias": 0.7301,
              Parameters: 87.8,
          },
          
  
          
          {
              Model: "Swin-L",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚óè Adversarial training",
              Text: "",
              Title: "A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking",
              Accuracy: 0.7874,
              "Adversarial Robustness": 0.6048,
              "Corruption Robustness": 0.6742,
              "OOD Robustness": 0.7,
              "Calibration Error": 0.0074,
              "Class Balance": 0.7945,
              "Object Focus": 0.9436,
              "Shape Bias": 0.7321,
              Parameters: 196.5,
          },
          
  
          
          {
              Model: "ConvNeXt-B",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚óè Adversarial training",
              Text: "",
              Title: "A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking",
              Accuracy: 0.7714,
              "Adversarial Robustness": 0.5678,
              "Corruption Robustness": 0.6357,
              "OOD Robustness": 0.621,
              "Calibration Error": 0.0075,
              "Class Balance": 0.7907,
              "Object Focus": 0.9465,
              "Shape Bias": 0.7264,
              Parameters: 88.6,
          },
          
  
          
          {
              Model: "ConvNeXt-L",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚óè Adversarial training",
              Text: "",
              Title: "A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking",
              Accuracy: 0.7812,
              "Adversarial Robustness": 0.5876,
              "Corruption Robustness": 0.6519,
              "OOD Robustness": 0.856,
              "Calibration Error": 0.0073,
              "Class Balance": 0.7942,
              "Object Focus": 0.9482,
              "Shape Bias": 0.7254,
              Parameters: 197.8,
          },
          
  
          
          {
              Model: "ConvNeXt-T",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚óè Adversarial training",
              Text: "",
              Title: "Revisiting Adversarial Training for {ImageNet}: Architectures, Training and Generalization across Threat Models",
              Accuracy: 0.7326,
              "Adversarial Robustness": 0.5055,
              "Corruption Robustness": 0.6021,
              "OOD Robustness": 0.6058,
              "Calibration Error": 0.0078,
              "Class Balance": 0.7768,
              "Object Focus": 0.9215,
              "Shape Bias": 0.72,
              Parameters: 28.6,
          },
          
  
          
          {
              Model: "ConvNeXt-S",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚óè Adversarial training",
              Text: "",
              Title: "Revisiting Adversarial Training for {ImageNet}: Architectures, Training and Generalization across Threat Models",
              Accuracy: 0.7428,
              "Adversarial Robustness": 0.523,
              "Corruption Robustness": 0.6084,
              "OOD Robustness": 0.7594,
              "Calibration Error": 0.0081,
              "Class Balance": 0.7822,
              "Object Focus": 0.9378,
              "Shape Bias": 0.7197,
              Parameters: 50.3,
          },
          
  
          
          {
              Model: "ConNeXt-B",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚óè Adversarial training",
              Text: "",
              Title: "Revisiting Adversarial Training for {ImageNet}: Architectures, Training and Generalization across Threat Models",
              Accuracy: 0.7607,
              "Adversarial Robustness": 0.5673,
              "Corruption Robustness": 0.6375,
              "OOD Robustness": 0.803,
              "Calibration Error": 0.0069,
              "Class Balance": 0.7836,
              "Object Focus": 0.9458,
              "Shape Bias": 0.7362,
              Parameters: 88.8,
          },
          
  
          
          {
              Model: "ConvNeXt-L",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚óè Adversarial training",
              Text: "",
              Title: "Revisiting Adversarial Training for {ImageNet}: Architectures, Training and Generalization across Threat Models",
              Accuracy: 0.7743,
              "Adversarial Robustness": 0.5947,
              "Corruption Robustness": 0.6552,
              "OOD Robustness": 0.493,
              "Calibration Error": 0.0069,
              "Class Balance": 0.7875,
              "Object Focus": 0.9506,
              "Shape Bias": 0.7442,
              Parameters: 198.1,
          },
          
  
          
          {
              Model: "ViT-B",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚óè Adversarial training",
              Text: "",
              Title: "Revisiting Adversarial Training for {ImageNet}: Architectures, Training and Generalization across Threat Models",
              Accuracy: 0.7672,
              "Adversarial Robustness": 0.5453,
              "Corruption Robustness": 0.6877,
              "OOD Robustness": 0.5021,
              "Calibration Error": 0.0072,
              "Class Balance": 0.7895,
              "Object Focus": 0.9504,
              "Shape Bias": 0.773,
              Parameters: 87.1,
          },
          
  
          
          {
              Model: "ConvNextV2-N",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "",
              Title: "ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders",
              Accuracy: 0.8186,
              "Adversarial Robustness": 0.1842,
              "Corruption Robustness": 0.6483,
              "OOD Robustness": 0.649,
              "Calibration Error": 0.0069,
              "Class Balance": 0.8028,
              "Object Focus": 0.9442,
              "Shape Bias": 0.2873,
              Parameters: 15.6,
          },
          
  
          
          {
              Model: "ConvNextV2-N",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚Ä¢",
              Title: "ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders",
              Accuracy: 0.8203,
              "Adversarial Robustness": 0.1183,
              "Corruption Robustness": 0.6849,
              "OOD Robustness": 0.6317,
              "Calibration Error": 0.0027,
              "Class Balance": 0.789,
              "Object Focus": 0.9404,
              "Shape Bias": 0.3159,
              Parameters: 15.6,
          },
          
  
          
          {
              Model: "ConvNextV2-T",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "",
              Title: "ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders",
              Accuracy: 0.8294,
              "Adversarial Robustness": 0.2006,
              "Corruption Robustness": 0.6943,
              "OOD Robustness": 0.6484,
              "Calibration Error": 0.0026,
              "Class Balance": 0.7961,
              "Object Focus": 0.9396,
              "Shape Bias": 0.3191,
              Parameters: 28.6,
          },
          
  
          
          {
              Model: "ConvNextV2-T",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚Ä¢",
              Title: "ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders",
              Accuracy: 0.8389,
              "Adversarial Robustness": 0.1633,
              "Corruption Robustness": 0.7214,
              "OOD Robustness": 0.7256,
              "Calibration Error": 0.0026,
              "Class Balance": 0.7989,
              "Object Focus": 0.9488,
              "Shape Bias": 0.3481,
              Parameters: 28.6,
          },
          
  
          
          {
              Model: "ConvNextV2-B",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "",
              Title: "ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders",
              Accuracy: 0.8488,
              "Adversarial Robustness": 0.2808,
              "Corruption Robustness": 0.7666,
              "OOD Robustness": 0.6973,
              "Calibration Error": 0.0033,
              "Class Balance": 0.8088,
              "Object Focus": 0.9567,
              "Shape Bias": 0.4229,
              Parameters: 88.7,
          },
          
  
          
          {
              Model: "ConvNextV2-B",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚Ä¢",
              Title: "ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders",
              Accuracy: 0.8674,
              "Adversarial Robustness": 0.2802,
              "Corruption Robustness": 0.7923,
              "OOD Robustness": 0.8185,
              "Calibration Error": 0.0023,
              "Class Balance": 0.8088,
              "Object Focus": 0.9607,
              "Shape Bias": 0.4023,
              Parameters: 88.7,
          },
          
  
          
          {
              Model: "ConvNextV2-L",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "",
              Title: "ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders",
              Accuracy: 0.8576,
              "Adversarial Robustness": 0.3508,
              "Corruption Robustness": 0.7889,
              "OOD Robustness": 0.7758,
              "Calibration Error": 0.0028,
              "Class Balance": 0.8135,
              "Object Focus": 0.9692,
              "Shape Bias": 0.4187,
              Parameters: 198.0,
          },
          
  
          
          {
              Model: "ConvNextV2-L",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚Ä¢",
              Title: "ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders",
              Accuracy: 0.8726,
              "Adversarial Robustness": 0.3182,
              "Corruption Robustness": 0.7979,
              "OOD Robustness": 0.8055,
              "Calibration Error": 0.0023,
              "Class Balance": 0.8135,
              "Object Focus": 0.9655,
              "Shape Bias": 0.4473,
              Parameters: 198.0,
          },
          
  
          
          {
              Model: "Hiera-T",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "",
              Title: "Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles",
              Accuracy: 0.8253,
              "Adversarial Robustness": 0.1878,
              "Corruption Robustness": 0.6963,
              "OOD Robustness": 0.6584,
              "Calibration Error": 0.0114,
              "Class Balance": 0.925,
              "Object Focus": 0.9282,
              "Shape Bias": 0.2702,
              Parameters: 27.9,
          },
          
  
          
          {
              Model: "Hiera-S",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "",
              Title: "Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles",
              Accuracy: 0.8369,
              "Adversarial Robustness": 0.1942,
              "Corruption Robustness": 0.7362,
              "OOD Robustness": 0.7002,
              "Calibration Error": 0.0118,
              "Class Balance": 0.9264,
              "Object Focus": 0.9298,
              "Shape Bias": 0.3247,
              Parameters: 35.0,
          },
          
  
          
          {
              Model: "Hiera-B",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "",
              Title: "Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles",
              Accuracy: 0.8455,
              "Adversarial Robustness": 0.2251,
              "Corruption Robustness": 0.7588,
              "OOD Robustness": 0.7607,
              "Calibration Error": 0.013,
              "Class Balance": 0.9279,
              "Object Focus": 0.9416,
              "Shape Bias": 0.3353,
              Parameters: 51.5,
          },
          
  
          
          {
              Model: "Hiera-B-Plus",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "",
              Title: "Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles",
              Accuracy: 0.8501,
              "Adversarial Robustness": 0.2369,
              "Corruption Robustness": 0.7818,
              "OOD Robustness": 0.7377,
              "Calibration Error": 0.013,
              "Class Balance": 0.929,
              "Object Focus": 0.9474,
              "Shape Bias": 0.4307,
              Parameters: 69.9,
          },
          
  
          
          {
              Model: "Hiera-L",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "",
              Title: "Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles",
              Accuracy: 0.8607,
              "Adversarial Robustness": 0.3243,
              "Corruption Robustness": 0.8126,
              "OOD Robustness": 0.7755,
              "Calibration Error": 0.0127,
              "Class Balance": 0.9305,
              "Object Focus": 0.955,
              "Shape Bias": 0.4187,
              Parameters: 213.7,
          },
          
  
          
          {
              Model: "EVA02-t",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚Ä¢",
              Title: "EVA-02: A Visual Representation for Neon Genesis",
              Accuracy: 0.8063,
              "Adversarial Robustness": 0.0986,
              "Corruption Robustness": 0.6468,
              "OOD Robustness": 0.5781,
              "Calibration Error": 0.0031,
              "Class Balance": 0.7873,
              "Object Focus": 0.9299,
              "Shape Bias": 0.205,
              Parameters: 5.8,
          },
          
  
          
          {
              Model: "EVA02-s",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚Ä¢",
              Title: "EVA-02: A Visual Representation for Neon Genesis",
              Accuracy: 0.8572,
              "Adversarial Robustness": 0.1731,
              "Corruption Robustness": 0.7351,
              "OOD Robustness": 0.7661,
              "Calibration Error": 0.003,
              "Class Balance": 0.8117,
              "Object Focus": 0.947,
              "Shape Bias": 0.2828,
              Parameters: 22.1,
          },
          
  
          
          {
              Model: "EVA02-b",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚Ä¢",
              Title: "EVA-02: A Visual Representation for Neon Genesis",
              Accuracy: 0.8826,
              "Adversarial Robustness": 0.2137,
              "Corruption Robustness": 0.8066,
              "OOD Robustness": 0.8635,
              "Calibration Error": 0.0039,
              "Class Balance": 0.832,
              "Object Focus": 0.9712,
              "Shape Bias": 0.335,
              Parameters: 87.1,
          },
          
  
          
          {
              Model: "InceptionNext-t",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "InceptionNeXt: When Inception Meets ConvNeXt",
              Accuracy: 0.8249,
              "Adversarial Robustness": 0.2104,
              "Corruption Robustness": 0.6778,
              "OOD Robustness": 0.6393,
              "Calibration Error": 0.0082,
              "Class Balance": 0.8134,
              "Object Focus": 0.9416,
              "Shape Bias": 0.3079,
              Parameters: 28.1,
          },
          
  
          
          {
              Model: "InceptionNext-s",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "InceptionNeXt: When Inception Meets ConvNeXt",
              Accuracy: 0.8359,
              "Adversarial Robustness": 0.2793,
              "Corruption Robustness": 0.7116,
              "OOD Robustness": 0.6654,
              "Calibration Error": 0.0072,
              "Class Balance": 0.8174,
              "Object Focus": 0.9459,
              "Shape Bias": 0.2852,
              Parameters: 49.4,
          },
          
  
          
          {
              Model: "InceptionNext-b",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "InceptionNeXt: When Inception Meets ConvNeXt",
              Accuracy: 0.8409,
              "Adversarial Robustness": 0.2842,
              "Corruption Robustness": 0.739,
              "OOD Robustness": 0.7158,
              "Calibration Error": 0.0081,
              "Class Balance": 0.8231,
              "Object Focus": 0.9439,
              "Shape Bias": 0.3501,
              Parameters: 86.7,
          },
          
  
          
          {
              Model: "FastViT-sa12",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization",
              Accuracy: 0.8085,
              "Adversarial Robustness": 0.1375,
              "Corruption Robustness": 0.6257,
              "OOD Robustness": 0.5696,
              "Calibration Error": 0.0065,
              "Class Balance": 0.7992,
              "Object Focus": 0.9389,
              "Shape Bias": 0.2075,
              Parameters: 11.6,
          },
          
  
          
          {
              Model: "FastViT-sa24",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization",
              Accuracy: 0.8267,
              "Adversarial Robustness": 0.241,
              "Corruption Robustness": 0.6909,
              "OOD Robustness": 0.6194,
              "Calibration Error": 0.0089,
              "Class Balance": 0.8169,
              "Object Focus": 0.9364,
              "Shape Bias": 0.2636,
              Parameters: 21.6,
          },
          
  
          
          {
              Model: "FastViT-sa36",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization",
              Accuracy: 0.835,
              "Adversarial Robustness": 0.269,
              "Corruption Robustness": 0.7118,
              "OOD Robustness": 0.6317,
              "Calibration Error": 0.0083,
              "Class Balance": 0.8207,
              "Object Focus": 0.945,
              "Shape Bias": 0.2622,
              Parameters: 31.5,
          },
          
  
          
          {
              Model: "BeiTV2-b",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "",
              Title: "BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers",
              Accuracy: 0.856,
              "Adversarial Robustness": 0.3313,
              "Corruption Robustness": 0.7937,
              "OOD Robustness": 0.5452,
              "Calibration Error": 0.0035,
              "Class Balance": 0.8128,
              "Object Focus": 0.9562,
              "Shape Bias": 0.4517,
              Parameters: 86.5,
          },
          
  
          
          {
              Model: "SeNet154",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Squeeze-and-Excitation Networks",
              Accuracy: 0.8123,
              "Adversarial Robustness": 0.3583,
              "Corruption Robustness": 0.0897,
              "OOD Robustness": 0.3471,
              "Calibration Error": 0.0091,
              "Class Balance": 0.804,
              "Object Focus": 0.9274,
              "Shape Bias": 0.3099,
              Parameters: 115.1,
          },
          
  
          
          {
              Model: "ResNet50d",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Bag of Tricks for Image Classification with Convolutional Neural Networks",
              Accuracy: 0.7908,
              "Adversarial Robustness": 0.2803,
              "Corruption Robustness": 0.046,
              "OOD Robustness": 0.3378,
              "Calibration Error": 0.0124,
              "Class Balance": 0.7936,
              "Object Focus": 0.9461,
              "Shape Bias": 0.2066,
              Parameters: 25.6,
          },
          
  
          
          {
              Model: "ResNeXt50-32x4d",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚óÜ Semi-supervised learning",
              Text: "‚òÖ",
              Title: "Billion-scale semi-supervised learning for image classification",
              Accuracy: 0.8031,
              "Adversarial Robustness": 0.1982,
              "Corruption Robustness": 0.0468,
              "OOD Robustness": 0.2391,
              "Calibration Error": 0.0088,
              "Class Balance": 0.783,
              "Object Focus": 0.9325,
              "Shape Bias": 0.2444,
              Parameters: 25.0,
          },
          
  
          
          {
              Model: "ResNet50",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚óÜ Semi-supervised learning",
              Text: "‚òÖ",
              Title: "Billion-scale semi-supervised learning for image classification",
              Accuracy: 0.7923,
              "Adversarial Robustness": 0.1568,
              "Corruption Robustness": 0.074,
              "OOD Robustness": 0.3262,
              "Calibration Error": 0.0022,
              "Class Balance": 0.7657,
              "Object Focus": 0.8976,
              "Shape Bias": 0.2174,
              Parameters: 25.6,
          },
          
  
          
          {
              Model: "ResNet50",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚óÜ Semi-supervised learning",
              Text: "‚òÖ",
              Title: "Billion-scale semi-supervised learning for image classification",
              Accuracy: 0.8114,
              "Adversarial Robustness": 0.2043,
              "Corruption Robustness": 0.0794,
              "OOD Robustness": 0.4373,
              "Calibration Error": 0.0022,
              "Class Balance": 0.7728,
              "Object Focus": 0.9275,
              "Shape Bias": 0.287,
              Parameters: 25.6,
          },
          
  
          
          {
              Model: "ResNeXt101-32x8d",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚óÜ Semi-supervised learning",
              Text: "‚òÖ",
              Title: "Billion-scale semi-supervised learning for image classification",
              Accuracy: 0.8428,
              "Adversarial Robustness": 0.3335,
              "Corruption Robustness": 0.06,
              "OOD Robustness": 0.298,
              "Calibration Error": 0.0121,
              "Class Balance": 0.8051,
              "Object Focus": 0.9458,
              "Shape Bias": 0.4235,
              Parameters: 88.8,
          },
          
  
          
          {
              Model: "ResNeXt50-32x4d",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚óÜ Semi-supervised learning",
              Text: "‚òÖ",
              Title: "Billion-scale semi-supervised learning for image classification",
              Accuracy: 0.8217,
              "Adversarial Robustness": 0.2522,
              "Corruption Robustness": 0.0567,
              "OOD Robustness": 0.2724,
              "Calibration Error": 0.0094,
              "Class Balance": 0.7926,
              "Object Focus": 0.9327,
              "Shape Bias": 0.283,
              Parameters: 25.0,
          },
          
  
          
          {
              Model: "ResNet18",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚óÜ Semi-supervised learning",
              Text: "‚òÖ",
              Title: "Billion-scale semi-supervised learning for image classification",
              Accuracy: 0.7329,
              "Adversarial Robustness": 0.114,
              "Corruption Robustness": 0.0639,
              "OOD Robustness": 0.3467,
              "Calibration Error": 0.006,
              "Class Balance": 0.7677,
              "Object Focus": 0.9049,
              "Shape Bias": 0.2718,
              Parameters: 11.7,
          },
          
  
          
          {
              Model: "ViT-b/16-MAE",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "",
              Title: "Masked Autoencoders Are Scalable Vision Learners",
              Accuracy: 0.8374,
              "Adversarial Robustness": 0.2529,
              "Corruption Robustness": 0.7053,
              "OOD Robustness": 0.5754,
              "Calibration Error": 0.0049,
              "Class Balance": 0.7972,
              "Object Focus": 0.9472,
              "Shape Bias": 0.3647,
              Parameters: 86.6,
          },
          
  
          
          {
              Model: "ViT-b/16-MAE",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "",
              Title: "Masked Autoencoders Are Scalable Vision Learners",
              Accuracy: 0.6675,
              "Adversarial Robustness": 0.0361,
              "Corruption Robustness": 0.4342,
              "OOD Robustness": 0.44,
              "Calibration Error": 0.0073,
              "Class Balance": 0.7914,
              "Object Focus": 0.8506,
              "Shape Bias": 0.2873,
              Parameters: 86.6,
          },
          
  
          
          {
              Model: "Hiera-B",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "",
              Title: "Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles",
              Accuracy: 0.6518,
              "Adversarial Robustness": 0.0572,
              "Corruption Robustness": 0.5844,
              "OOD Robustness": 0.6562,
              "Calibration Error": 0.003,
              "Class Balance": 0.7484,
              "Object Focus": 0.8451,
              "Shape Bias": 0.3516,
              Parameters: 51.5,
          },
          
  
          
          {
              Model: "ViT-b/16-DINO",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "",
              Title: "Emerging Properties in Self-Supervised Vision Transformers",
              Accuracy: 0.7806,
              "Adversarial Robustness": 0.1344,
              "Corruption Robustness": 0.6562,
              "OOD Robustness": 0.4003,
              "Calibration Error": 0.0012,
              "Class Balance": 0.7674,
              "Object Focus": 0.9105,
              "Shape Bias": 0.3602,
              Parameters: 87.3,
          },
          
  
          
          {
              Model: "ViT-b/16-DINO",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "",
              Title: "Emerging Properties in Self-Supervised Vision Transformers",
              Accuracy: 0.8286,
              "Adversarial Robustness": 0.2889,
              "Corruption Robustness": 0.755,
              "OOD Robustness": 0.619,
              "Calibration Error": 0.0039,
              "Class Balance": 0.8019,
              "Object Focus": 0.9428,
              "Shape Bias": 0.4229,
              Parameters: 86.6,
          },
          
  
          
          {
              Model: "ResNet50-DINO",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "",
              Title: "Emerging Properties in Self-Supervised Vision Transformers",
              Accuracy: 0.7529,
              "Adversarial Robustness": 0.1747,
              "Corruption Robustness": 0.0257,
              "OOD Robustness": 0.1678,
              "Calibration Error": 0.0084,
              "Class Balance": 0.7983,
              "Object Focus": 0.6898,
              "Shape Bias": 0.2004,
              Parameters: 25.6,
          },
          
  
          
          {
              Model: "ResNet50-DINO",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "",
              Title: "Emerging Properties in Self-Supervised Vision Transformers",
              Accuracy: 0.7846,
              "Adversarial Robustness": 0.4516,
              "Corruption Robustness": 0.0664,
              "OOD Robustness": 0.3195,
              "Calibration Error": 0.0201,
              "Class Balance": 0.8363,
              "Object Focus": 0.923,
              "Shape Bias": 0.2266,
              Parameters: 25.6,
          },
          
  
          
          {
              Model: "ViT-l-14-dinoV2-LP",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "DINOv2: Learning Robust Visual Features without Supervision",
              Accuracy: 0.864,
              "Adversarial Robustness": 0.1941,
              "Corruption Robustness": 0.8617,
              "OOD Robustness": 0.8667,
              "Calibration Error": 0.001,
              "Class Balance": 0.8087,
              "Object Focus": 0.9426,
              "Shape Bias": 0.599,
              Parameters: 309.5,
          },
          
  
          
          {
              Model: "ViT-b-14-dinoV2",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "DINOv2: Learning Robust Visual Features without Supervision",
              Accuracy: 0.8453,
              "Adversarial Robustness": 0.1042,
              "Corruption Robustness": 0.7741,
              "OOD Robustness": 0.5596,
              "Calibration Error": 0.0011,
              "Class Balance": 0.7968,
              "Object Focus": 0.939,
              "Shape Bias": 0.4462,
              Parameters: 90.4,
          },
          
  
          
          {
              Model: "ViT-s-14-dinoV2-LP",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "DINOv2: Learning Robust Visual Features without Supervision",
              Accuracy: 0.8139,
              "Adversarial Robustness": 0.0657,
              "Corruption Robustness": 0.6807,
              "OOD Robustness": 0.7554,
              "Calibration Error": 0.001,
              "Class Balance": 0.7817,
              "Object Focus": 0.9233,
              "Shape Bias": 0.3624,
              Parameters: 24.0,
          },
          
  
          
          {
              Model: "ViT-t/16",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚óè ImageNet-21k",
              Setup: " ‚ñ† Supervised learning",
              Text: "‚Ä¢",
              Title: "DINOv2: Learning Robust Visual Features without Supervision",
              Accuracy: 0.7547,
              "Adversarial Robustness": 0.0079,
              "Corruption Robustness": 0.5637,
              "OOD Robustness": 0.5441,
              "Calibration Error": 0.0007,
              "Class Balance": 0.7604,
              "Object Focus": 0.8652,
              "Shape Bias": 0.2651,
              Parameters: 5.7,
          },
          
  
          
          {
              Model: "SigLIP-b/16",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "Sigmoid Loss for Language Image Pre-Training",
              Accuracy: 0.7607,
              "Adversarial Robustness": 0.0985,
              "Corruption Robustness": 0.6263,
              "OOD Robustness": 1.0563,
              "Calibration Error": 0.0373,
              "Class Balance": 0.8996,
              "Object Focus": 0.9487,
              "Shape Bias": 0.4868,
              Parameters: 203.2,
          },
          
  
          
          {
              Model: "CLIP-ResNet50",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "Learning Transferable Visual Models From Natural Language Supervision",
              Accuracy: 0.5981,
              "Adversarial Robustness": 0.2872,
              "Corruption Robustness": 0.4458,
              "OOD Robustness": 0.9957,
              "Calibration Error": 0.0109,
              "Class Balance": 0.938,
              "Object Focus": 0.8638,
              "Shape Bias": 0.1952,
              Parameters: 102.0,
          },
          
  
          
          {
              Model: "CLIP-b/16",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "Learning Transferable Visual Models From Natural Language Supervision",
              Accuracy: 0.6836,
              "Adversarial Robustness": 0.0322,
              "Corruption Robustness": 0.6415,
              "OOD Robustness": 1.1058,
              "Calibration Error": 0.0288,
              "Class Balance": 0.8835,
              "Object Focus": 0.9151,
              "Shape Bias": 0.4795,
              Parameters: 149.6,
          },
          
  
          
          {
              Model: "CLIP-ResNet101",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "Learning Transferable Visual Models From Natural Language Supervision",
              Accuracy: 0.6231,
              "Adversarial Robustness": 0.2631,
              "Corruption Robustness": 0.0411,
              "OOD Robustness": 0.5339,
              "Calibration Error": 0.0257,
              "Class Balance": 0.8799,
              "Object Focus": 0.9065,
              "Shape Bias": 0.277,
              Parameters: 119.7,
          },
          
  
          
          {
              Model: "CLIP-b/32",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "Learning Transferable Visual Models From Natural Language Supervision",
              Accuracy: 0.6338,
              "Adversarial Robustness": 0.0482,
              "Corruption Robustness": 0.6361,
              "OOD Robustness": 1.1204,
              "Calibration Error": 0.0318,
              "Class Balance": 0.8812,
              "Object Focus": 0.8617,
              "Shape Bias": 0.5801,
              Parameters: 151.3,
          },
          
  
          
          {
              Model: "MobileCLIP-S0",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training",
              Accuracy: 0.6773,
              "Adversarial Robustness": 0.0025,
              "Corruption Robustness": 0.1505,
              "OOD Robustness": 0.8828,
              "Calibration Error": 0.0325,
              "Class Balance": 0.8876,
              "Object Focus": 0.8843,
              "Shape Bias": 0.6461,
              Parameters: 53.8,
          },
          
  
          
          {
              Model: "MobileCLIP-S1",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training",
              Accuracy: 0.726,
              "Adversarial Robustness": 0.0033,
              "Corruption Robustness": 0.19,
              "OOD Robustness": 0.6812,
              "Calibration Error": 0.0344,
              "Class Balance": 0.8904,
              "Object Focus": 0.9203,
              "Shape Bias": 0.6477,
              Parameters: 84.9,
          },
          
  
          
          {
              Model: "MobileCLIP-S2",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training",
              Accuracy: 0.7442,
              "Adversarial Robustness": 0.0059,
              "Corruption Robustness": 0.177,
              "OOD Robustness": 0.7388,
              "Calibration Error": 0.0351,
              "Class Balance": 0.8911,
              "Object Focus": 0.9217,
              "Shape Bias": 0.6851,
              Parameters: 99.1,
          },
          
  
          
          {
              Model: "MobileCLIP-B",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training",
              Accuracy: 0.7676,
              "Adversarial Robustness": 0.0101,
              "Corruption Robustness": 0.6606,
              "OOD Robustness": 1.076,
              "Calibration Error": 0.0372,
              "Class Balance": 0.8965,
              "Object Focus": 0.9409,
              "Shape Bias": 0.6383,
              Parameters: 149.8,
          },
          
  
          
          {
              Model: "EfficientNet-B0",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.7707,
              "Adversarial Robustness": 0.2058,
              "Corruption Robustness": 0.024,
              "OOD Robustness": 0.2372,
              "Calibration Error": 0.0027,
              "Class Balance": 0.7376,
              "Object Focus": 0.9067,
              "Shape Bias": 0.2598,
              Parameters: 5.3,
          },
          
  
          
          {
              Model: "EfficientNet-B1",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.7921,
              "Adversarial Robustness": 0.2681,
              "Corruption Robustness": 0.0342,
              "OOD Robustness": 0.2987,
              "Calibration Error": 0.0022,
              "Class Balance": 0.7388,
              "Object Focus": 0.9163,
              "Shape Bias": 0.2673,
              Parameters: 7.8,
          },
          
  
          
          {
              Model: "EfficientNet-B2",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8023,
              "Adversarial Robustness": 0.2852,
              "Corruption Robustness": 0.0371,
              "OOD Robustness": 0.2782,
              "Calibration Error": 0.0021,
              "Class Balance": 0.7429,
              "Object Focus": 0.9266,
              "Shape Bias": 0.264,
              Parameters: 9.1,
          },
          
  
          
          {
              Model: "EfficientNet-B3",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.815,
              "Adversarial Robustness": 0.3289,
              "Corruption Robustness": 0.0528,
              "OOD Robustness": 0.3275,
              "Calibration Error": 0.0023,
              "Class Balance": 0.749,
              "Object Focus": 0.9407,
              "Shape Bias": 0.281,
              Parameters: 12.2,
          },
          
  
          
          {
              Model: "EfficientNet-B4",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8163,
              "Adversarial Robustness": 0.3772,
              "Corruption Robustness": 0.0902,
              "OOD Robustness": 0.3363,
              "Calibration Error": 0.0028,
              "Class Balance": 0.7604,
              "Object Focus": 0.9486,
              "Shape Bias": 0.2372,
              Parameters: 19.3,
          },
          
  
          
          {
              Model: "EfficientNet-v2-M",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8023,
              "Adversarial Robustness": 0.4554,
              "Corruption Robustness": 0.153,
              "OOD Robustness": 0.3603,
              "Calibration Error": 0.0036,
              "Class Balance": 0.7586,
              "Object Focus": 0.9226,
              "Shape Bias": 0.287,
              Parameters: 53.2,
          },
          
  
          
          {
              Model: "EfficientNet-v2-S",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8131,
              "Adversarial Robustness": 0.379,
              "Corruption Robustness": 0.0669,
              "OOD Robustness": 0.3252,
              "Calibration Error": 0.0029,
              "Class Balance": 0.7509,
              "Object Focus": 0.9289,
              "Shape Bias": 0.2676,
              Parameters: 23.9,
          },
          
  
          
          {
              Model: "RegNet-y-4gf",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8132,
              "Adversarial Robustness": 0.5634,
              "Corruption Robustness": 0.0411,
              "OOD Robustness": 0.282,
              "Calibration Error": 0.0022,
              "Class Balance": 0.7468,
              "Object Focus": 0.9396,
              "Shape Bias": 0.2793,
              Parameters: 20.6,
          },
          
  
          
          {
              Model: "RegNet-y-8gf",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.82,
              "Adversarial Robustness": 0.6239,
              "Corruption Robustness": 0.0604,
              "OOD Robustness": 0.2983,
              "Calibration Error": 0.0023,
              "Class Balance": 0.7531,
              "Object Focus": 0.938,
              "Shape Bias": 0.2524,
              Parameters: 39.2,
          },
          
  
          
          {
              Model: "RegNet-y-16gf",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8184,
              "Adversarial Robustness": 0.6365,
              "Corruption Robustness": 0.0628,
              "OOD Robustness": 0.2768,
              "Calibration Error": 0.0028,
              "Class Balance": 0.7531,
              "Object Focus": 0.9452,
              "Shape Bias": 0.2789,
              Parameters: 83.6,
          },
          
  
          
          {
              Model: "RegNet-y-32gf",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8241,
              "Adversarial Robustness": 0.6832,
              "Corruption Robustness": 0.1013,
              "OOD Robustness": 0.3434,
              "Calibration Error": 0.0029,
              "Class Balance": 0.7576,
              "Object Focus": 0.942,
              "Shape Bias": 0.2807,
              Parameters: 145.0,
          },
          
  
          
          {
              Model: "ResNet101",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.815,
              "Adversarial Robustness": 0.6053,
              "Corruption Robustness": 0.0677,
              "OOD Robustness": 0.3451,
              "Calibration Error": 0.0028,
              "Class Balance": 0.7361,
              "Object Focus": 0.9188,
              "Shape Bias": 0.2687,
              Parameters: 44.5,
          },
          
  
          
          {
              Model: "ResNet152",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8196,
              "Adversarial Robustness": 0.63,
              "Corruption Robustness": 0.0721,
              "OOD Robustness": 0.343,
              "Calibration Error": 0.0028,
              "Class Balance": 0.7359,
              "Object Focus": 0.9381,
              "Shape Bias": 0.3149,
              Parameters: 60.2,
          },
          
  
          
          {
              Model: "ResNet18",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.715,
              "Adversarial Robustness": 0.2737,
              "Corruption Robustness": 0.0535,
              "OOD Robustness": 0.3237,
              "Calibration Error": 0.0052,
              "Class Balance": 0.7419,
              "Object Focus": 0.8748,
              "Shape Bias": 0.2357,
              Parameters: 11.7,
          },
          
  
          
          {
              Model: "ResNet34",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.7642,
              "Adversarial Robustness": 0.4659,
              "Corruption Robustness": 0.0559,
              "OOD Robustness": 0.3571,
              "Calibration Error": 0.0051,
              "Class Balance": 0.7377,
              "Object Focus": 0.8958,
              "Shape Bias": 0.2888,
              Parameters: 21.8,
          },
          
  
          
          {
              Model: "ResNet50",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8038,
              "Adversarial Robustness": 0.5541,
              "Corruption Robustness": 0.0541,
              "OOD Robustness": 0.3174,
              "Calibration Error": 0.0034,
              "Class Balance": 0.7319,
              "Object Focus": 0.8943,
              "Shape Bias": 0.2148,
              Parameters: 25.6,
          },
          
  
          
          {
              Model: "ResNet50d",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8057,
              "Adversarial Robustness": 0.3984,
              "Corruption Robustness": 0.0538,
              "OOD Robustness": 0.2726,
              "Calibration Error": 0.0028,
              "Class Balance": 0.7397,
              "Object Focus": 0.9347,
              "Shape Bias": 0.2067,
              Parameters: 25.6,
          },
          
  
          
          {
              Model: "ResNeXt50-32x4d",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8053,
              "Adversarial Robustness": 0.588,
              "Corruption Robustness": 0.0463,
              "OOD Robustness": 0.3078,
              "Calibration Error": 0.0034,
              "Class Balance": 0.7336,
              "Object Focus": 0.917,
              "Shape Bias": 0.3028,
              Parameters: 25.0,
          },
          
  
          
          {
              Model: "SeNet154",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8152,
              "Adversarial Robustness": 0.6936,
              "Corruption Robustness": 0.0802,
              "OOD Robustness": 0.323,
              "Calibration Error": 0.0035,
              "Class Balance": 0.7535,
              "Object Focus": 0.9317,
              "Shape Bias": 0.3015,
              Parameters: 115.1,
          },
          
  
          
          {
              Model: "EfficientNet-B0",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.7698,
              "Adversarial Robustness": 0.1806,
              "Corruption Robustness": 0.0205,
              "OOD Robustness": 0.2318,
              "Calibration Error": 0.0026,
              "Class Balance": 0.7361,
              "Object Focus": 0.9006,
              "Shape Bias": 0.183,
              Parameters: 5.3,
          },
          
  
          
          {
              Model: "EfficientNet-B1",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.7938,
              "Adversarial Robustness": 0.1927,
              "Corruption Robustness": 0.0298,
              "OOD Robustness": 0.2568,
              "Calibration Error": 0.0021,
              "Class Balance": 0.7415,
              "Object Focus": 0.9156,
              "Shape Bias": 0.226,
              Parameters: 7.8,
          },
          
  
          
          {
              Model: "EfficientNet-B2",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8001,
              "Adversarial Robustness": 0.2533,
              "Corruption Robustness": 0.027,
              "OOD Robustness": 0.2492,
              "Calibration Error": 0.0033,
              "Class Balance": 0.7417,
              "Object Focus": 0.9326,
              "Shape Bias": 0.2379,
              Parameters: 9.1,
          },
          
  
          
          {
              Model: "EfficientNet-B3",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8131,
              "Adversarial Robustness": 0.2724,
              "Corruption Robustness": 0.0379,
              "OOD Robustness": 0.2785,
              "Calibration Error": 0.0022,
              "Class Balance": 0.7463,
              "Object Focus": 0.9463,
              "Shape Bias": 0.2422,
              Parameters: 12.2,
          },
          
  
          
          {
              Model: "EfficientNet-B4",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8239,
              "Adversarial Robustness": 0.3442,
              "Corruption Robustness": 0.062,
              "OOD Robustness": 0.2987,
              "Calibration Error": 0.0023,
              "Class Balance": 0.7573,
              "Object Focus": 0.9348,
              "Shape Bias": 0.2771,
              Parameters: 19.3,
          },
          
  
          
          {
              Model: "EfficientNet-v2-M",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8128,
              "Adversarial Robustness": 0.3908,
              "Corruption Robustness": 0.1024,
              "OOD Robustness": 0.364,
              "Calibration Error": 0.0032,
              "Class Balance": 0.7642,
              "Object Focus": 0.9295,
              "Shape Bias": 0.3186,
              Parameters: 53.2,
          },
          
  
          
          {
              Model: "EfficientNet-v2-S",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8188,
              "Adversarial Robustness": 0.3195,
              "Corruption Robustness": 0.0611,
              "OOD Robustness": 0.337,
              "Calibration Error": 0.0026,
              "Class Balance": 0.758,
              "Object Focus": 0.9403,
              "Shape Bias": 0.2788,
              Parameters: 23.9,
          },
          
  
          
          {
              Model: "RegNet-y-4gf",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8101,
              "Adversarial Robustness": 0.5495,
              "Corruption Robustness": 0.0378,
              "OOD Robustness": 0.2841,
              "Calibration Error": 0.0021,
              "Class Balance": 0.7477,
              "Object Focus": 0.9369,
              "Shape Bias": 0.2412,
              Parameters: 20.6,
          },
          
  
          
          {
              Model: "RegNet-y-8gf",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8181,
              "Adversarial Robustness": 0.5758,
              "Corruption Robustness": 0.0541,
              "OOD Robustness": 0.2866,
              "Calibration Error": 0.0024,
              "Class Balance": 0.7519,
              "Object Focus": 0.9366,
              "Shape Bias": 0.2596,
              Parameters: 39.2,
          },
          
  
          
          {
              Model: "RegNet-y-16gf",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8202,
              "Adversarial Robustness": 0.5827,
              "Corruption Robustness": 0.0589,
              "OOD Robustness": 0.2964,
              "Calibration Error": 0.0025,
              "Class Balance": 0.7561,
              "Object Focus": 0.9514,
              "Shape Bias": 0.3003,
              Parameters: 83.6,
          },
          
  
          
          {
              Model: "RegNet-y-32gf",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8212,
              "Adversarial Robustness": 0.5059,
              "Corruption Robustness": 0.0898,
              "OOD Robustness": 0.3138,
              "Calibration Error": 0.0029,
              "Class Balance": 0.7606,
              "Object Focus": 0.9438,
              "Shape Bias": 0.3081,
              Parameters: 145.0,
          },
          
  
          
          {
              Model: "ResNet101",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8132,
              "Adversarial Robustness": 0.4914,
              "Corruption Robustness": 0.0721,
              "OOD Robustness": 0.3152,
              "Calibration Error": 0.0026,
              "Class Balance": 0.7457,
              "Object Focus": 0.9274,
              "Shape Bias": 0.2473,
              Parameters: 44.5,
          },
          
  
          
          {
              Model: "ResNet152",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8177,
              "Adversarial Robustness": 0.5279,
              "Corruption Robustness": 0.0757,
              "OOD Robustness": 0.3298,
              "Calibration Error": 0.0029,
              "Class Balance": 0.7484,
              "Object Focus": 0.9291,
              "Shape Bias": 0.2343,
              Parameters: 60.2,
          },
          
  
          
          {
              Model: "ResNet18",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.7061,
              "Adversarial Robustness": 0.2599,
              "Corruption Robustness": 0.0476,
              "OOD Robustness": 0.3277,
              "Calibration Error": 0.0022,
              "Class Balance": 0.7286,
              "Object Focus": 0.8901,
              "Shape Bias": 0.1955,
              Parameters: 11.7,
          },
          
  
          
          {
              Model: "ResNet34",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.7552,
              "Adversarial Robustness": 0.3755,
              "Corruption Robustness": 0.053,
              "OOD Robustness": 0.3687,
              "Calibration Error": 0.0026,
              "Class Balance": 0.7294,
              "Object Focus": 0.9122,
              "Shape Bias": 0.2338,
              Parameters: 21.8,
          },
          
  
          
          {
              Model: "ResNet50",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.7984,
              "Adversarial Robustness": 0.4933,
              "Corruption Robustness": 0.0616,
              "OOD Robustness": 0.3135,
              "Calibration Error": 0.0027,
              "Class Balance": 0.7381,
              "Object Focus": 0.9234,
              "Shape Bias": 0.1869,
              Parameters: 25.6,
          },
          
  
          
          {
              Model: "ResNet50d",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8021,
              "Adversarial Robustness": 0.462,
              "Corruption Robustness": 0.0516,
              "OOD Robustness": 0.2634,
              "Calibration Error": 0.0027,
              "Class Balance": 0.7403,
              "Object Focus": 0.9389,
              "Shape Bias": 0.2022,
              Parameters: 25.6,
          },
          
  
          
          {
              Model: "ResNeXt50-32x4d",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8031,
              "Adversarial Robustness": 0.5632,
              "Corruption Robustness": 0.0545,
              "OOD Robustness": 0.2708,
              "Calibration Error": 0.0026,
              "Class Balance": 0.7406,
              "Object Focus": 0.9412,
              "Shape Bias": 0.1951,
              Parameters: 25.0,
          },
          
  
          
          {
              Model: "SeNet154",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8168,
              "Adversarial Robustness": 0.4569,
              "Corruption Robustness": 0.0717,
              "OOD Robustness": 0.2916,
              "Calibration Error": 0.0032,
              "Class Balance": 0.7559,
              "Object Focus": 0.9401,
              "Shape Bias": 0.3048,
              Parameters: 115.1,
          },
          
  
          
          {
              Model: "EfficientNet-B0",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.7289,
              "Adversarial Robustness": 0.1696,
              "Corruption Robustness": 0.0161,
              "OOD Robustness": 0.2359,
              "Calibration Error": 0.0054,
              "Class Balance": 0.7493,
              "Object Focus": 0.8647,
              "Shape Bias": 0.1582,
              Parameters: 5.3,
          },
          
  
          
          {
              Model: "EfficientNet-B1",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.7371,
              "Adversarial Robustness": 0.1121,
              "Corruption Robustness": 0.0189,
              "OOD Robustness": 0.2195,
              "Calibration Error": 0.0048,
              "Class Balance": 0.7472,
              "Object Focus": 0.8799,
              "Shape Bias": 0.1379,
              Parameters: 7.8,
          },
          
  
          
          {
              Model: "EfficientNet-B2",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.7656,
              "Adversarial Robustness": 0.1574,
              "Corruption Robustness": 0.0198,
              "OOD Robustness": 0.1991,
              "Calibration Error": 0.004,
              "Class Balance": 0.7455,
              "Object Focus": 0.9053,
              "Shape Bias": 0.1502,
              Parameters: 9.1,
          },
          
  
          
          {
              Model: "EfficientNet-B3",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.7826,
              "Adversarial Robustness": 0.2207,
              "Corruption Robustness": 0.0212,
              "OOD Robustness": 0.21,
              "Calibration Error": 0.0137,
              "Class Balance": 0.7921,
              "Object Focus": 0.8942,
              "Shape Bias": 0.1467,
              Parameters: 12.2,
          },
          
  
          
          {
              Model: "EfficientNet-B4",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8122,
              "Adversarial Robustness": 0.303,
              "Corruption Robustness": 0.0545,
              "OOD Robustness": 0.274,
              "Calibration Error": 0.0024,
              "Class Balance": 0.7581,
              "Object Focus": 0.9268,
              "Shape Bias": 0.2155,
              Parameters: 19.3,
          },
          
  
          
          {
              Model: "EfficientNet-v2-M",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8229,
              "Adversarial Robustness": 0.26,
              "Corruption Robustness": 0.0759,
              "OOD Robustness": 0.3575,
              "Calibration Error": 0.0022,
              "Class Balance": 0.7648,
              "Object Focus": 0.9357,
              "Shape Bias": 0.2273,
              Parameters: 53.2,
          },
          
  
          
          {
              Model: "EfficientNet-v2-S",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8091,
              "Adversarial Robustness": 0.1997,
              "Corruption Robustness": 0.0462,
              "OOD Robustness": 0.3111,
              "Calibration Error": 0.0022,
              "Class Balance": 0.7561,
              "Object Focus": 0.91,
              "Shape Bias": 0.1684,
              Parameters: 23.9,
          },
          
  
          
          {
              Model: "RegNet-y-4gf",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.7858,
              "Adversarial Robustness": 0.4081,
              "Corruption Robustness": 0.0308,
              "OOD Robustness": 0.2747,
              "Calibration Error": 0.005,
              "Class Balance": 0.7506,
              "Object Focus": 0.9416,
              "Shape Bias": 0.1648,
              Parameters: 20.6,
          },
          
  
          
          {
              Model: "RegNet-y-8gf",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8086,
              "Adversarial Robustness": 0.4629,
              "Corruption Robustness": 0.0417,
              "OOD Robustness": 0.2709,
              "Calibration Error": 0.003,
              "Class Balance": 0.7526,
              "Object Focus": 0.9368,
              "Shape Bias": 0.189,
              Parameters: 39.2,
          },
          
  
          
          {
              Model: "RegNet-y-16gf",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8129,
              "Adversarial Robustness": 0.4677,
              "Corruption Robustness": 0.043,
              "OOD Robustness": 0.2678,
              "Calibration Error": 0.0024,
              "Class Balance": 0.755,
              "Object Focus": 0.9386,
              "Shape Bias": 0.1651,
              Parameters: 83.6,
          },
          
  
          
          {
              Model: "RegNet-y-32gf",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8229,
              "Adversarial Robustness": 0.5208,
              "Corruption Robustness": 0.066,
              "OOD Robustness": 0.2971,
              "Calibration Error": 0.0022,
              "Class Balance": 0.7607,
              "Object Focus": 0.9372,
              "Shape Bias": 0.1822,
              Parameters: 145.0,
          },
          
  
          
          {
              Model: "ResNet101",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.7982,
              "Adversarial Robustness": 0.3923,
              "Corruption Robustness": 0.0608,
              "OOD Robustness": 0.2877,
              "Calibration Error": 0.0067,
              "Class Balance": 0.7516,
              "Object Focus": 0.9264,
              "Shape Bias": 0.1659,
              Parameters: 44.5,
          },
          
  
          
          {
              Model: "ResNet152",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8055,
              "Adversarial Robustness": 0.4004,
              "Corruption Robustness": 0.0612,
              "OOD Robustness": 0.2968,
              "Calibration Error": 0.0078,
              "Class Balance": 0.7569,
              "Object Focus": 0.9197,
              "Shape Bias": 0.1896,
              Parameters: 60.2,
          },
          
  
          
          {
              Model: "ResNet18",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.6825,
              "Adversarial Robustness": 0.1748,
              "Corruption Robustness": 0.0359,
              "OOD Robustness": 0.3137,
              "Calibration Error": 0.0057,
              "Class Balance": 0.7512,
              "Object Focus": 0.8267,
              "Shape Bias": 0.1425,
              Parameters: 11.7,
          },
          
  
          
          {
              Model: "ResNet34",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.7297,
              "Adversarial Robustness": 0.2698,
              "Corruption Robustness": 0.0364,
              "OOD Robustness": 0.2886,
              "Calibration Error": 0.0051,
              "Class Balance": 0.7492,
              "Object Focus": 0.8649,
              "Shape Bias": 0.1805,
              Parameters: 21.8,
          },
          
  
          
          {
              Model: "ResNet50",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.7805,
              "Adversarial Robustness": 0.3354,
              "Corruption Robustness": 0.0469,
              "OOD Robustness": 0.3083,
              "Calibration Error": 0.0063,
              "Class Balance": 0.7479,
              "Object Focus": 0.9136,
              "Shape Bias": 0.1353,
              Parameters: 25.6,
          },
          
  
          
          {
              Model: "ResNet50d",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.785,
              "Adversarial Robustness": 0.2679,
              "Corruption Robustness": 0.042,
              "OOD Robustness": 0.2439,
              "Calibration Error": 0.003,
              "Class Balance": 0.7466,
              "Object Focus": 0.9267,
              "Shape Bias": 0.1397,
              Parameters: 25.6,
          },
          
  
          
          {
              Model: "ResNeXt50-32x4d",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.7884,
              "Adversarial Robustness": 0.5016,
              "Corruption Robustness": 0.0373,
              "OOD Robustness": 0.2318,
              "Calibration Error": 0.0075,
              "Class Balance": 0.7545,
              "Object Focus": 0.9033,
              "Shape Bias": 0.1335,
              Parameters: 25.0,
          },
          
  
          
          {
              Model: "SeNet154",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚¨ü A[1,2,3]",
              Text: "",
              Title: "ResNet strikes back: An improved training procedure in timm",
              Accuracy: 0.8159,
              "Adversarial Robustness": 0.4997,
              "Corruption Robustness": 0.055,
              "OOD Robustness": 0.2576,
              "Calibration Error": 0.0025,
              "Class Balance": 0.7562,
              "Object Focus": 0.9362,
              "Shape Bias": 0.1812,
              Parameters: 115.1,
          },
          
  
          
          {
              Model: "ConvNeXt-T",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#59a14f" stroke="#59a14f" stroke-width="1" opacity="0.7"/></svg> Bcos models',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "B-cos Networks: Alignment is All We Need for Interpretability",
              Accuracy: 0.7948,
              "Adversarial Robustness": 0.0279,
              "Corruption Robustness": 0.5691,
              "OOD Robustness": 0.5388,
              "Calibration Error": 0.0161,
              "Class Balance": 0.7734,
              "Object Focus": 0.9369,
              "Shape Bias": 0.2325,
              Parameters: 88.5,
          },
          
  
          
          {
              Model: "ConNeXt-B",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#59a14f" stroke="#59a14f" stroke-width="1" opacity="0.7"/></svg> Bcos models',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "B-cos Networks: Alignment is All We Need for Interpretability",
              Accuracy: 0.771,
              "Adversarial Robustness": 0.0212,
              "Corruption Robustness": 0.5202,
              "OOD Robustness": 0.5176,
              "Calibration Error": 0.0117,
              "Class Balance": 0.7603,
              "Object Focus": 0.8751,
              "Shape Bias": 0.1971,
              Parameters: 28.5,
          },
          
  
          
          {
              Model: "DenseNet121",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#59a14f" stroke="#59a14f" stroke-width="1" opacity="0.7"/></svg> Bcos models',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "B-cos Networks: Alignment is All We Need for Interpretability",
              Accuracy: 0.736,
              "Adversarial Robustness": 0.0076,
              "Corruption Robustness": 0.1705,
              "OOD Robustness": 0.4381,
              "Calibration Error": 0.0099,
              "Class Balance": 0.7419,
              "Object Focus": 0.9024,
              "Shape Bias": 0.2276,
              Parameters: 7.9,
          },
          
  
          
          {
              Model: "DenseNet161",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#59a14f" stroke="#59a14f" stroke-width="1" opacity="0.7"/></svg> Bcos models',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "B-cos Networks: Alignment is All We Need for Interpretability",
              Accuracy: 0.7663,
              "Adversarial Robustness": 0.0174,
              "Corruption Robustness": 0.181,
              "OOD Robustness": 0.4301,
              "Calibration Error": 0.0069,
              "Class Balance": 0.73,
              "Object Focus": 0.9159,
              "Shape Bias": 0.2861,
              Parameters: 28.6,
          },
          
  
          
          {
              Model: "DenseNet169",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#59a14f" stroke="#59a14f" stroke-width="1" opacity="0.7"/></svg> Bcos models',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "B-cos Networks: Alignment is All We Need for Interpretability",
              Accuracy: 0.7518,
              "Adversarial Robustness": 0.0159,
              "Corruption Robustness": 0.1722,
              "OOD Robustness": 0.4289,
              "Calibration Error": 0.0088,
              "Class Balance": 0.7351,
              "Object Focus": 0.9051,
              "Shape Bias": 0.251,
              Parameters: 14.1,
          },
          
  
          
          {
              Model: "DenseNet201",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#59a14f" stroke="#59a14f" stroke-width="1" opacity="0.7"/></svg> Bcos models',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "B-cos Networks: Alignment is All We Need for Interpretability",
              Accuracy: 0.7548,
              "Adversarial Robustness": 0.0018,
              "Corruption Robustness": 0.1774,
              "OOD Robustness": 0.485,
              "Calibration Error": 0.0039,
              "Class Balance": 0.7369,
              "Object Focus": 0.9091,
              "Shape Bias": 0.2722,
              Parameters: 19.9,
          },
          
  
          
          {
              Model: "ResNet152",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#59a14f" stroke="#59a14f" stroke-width="1" opacity="0.7"/></svg> Bcos models',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "B-cos Networks: Alignment is All We Need for Interpretability",
              Accuracy: 0.7649,
              "Adversarial Robustness": 0.0881,
              "Corruption Robustness": 0.1447,
              "OOD Robustness": 0.4029,
              "Calibration Error": 0.0207,
              "Class Balance": 0.7711,
              "Object Focus": 0.908,
              "Shape Bias": 0.3354,
              Parameters: 60.1,
          },
          
  
          
          {
              Model: "ResNet18",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#59a14f" stroke="#59a14f" stroke-width="1" opacity="0.7"/></svg> Bcos models',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "B-cos Networks: Alignment is All We Need for Interpretability",
              Accuracy: 0.6872,
              "Adversarial Robustness": 0.0011,
              "Corruption Robustness": 0.1112,
              "OOD Robustness": 0.4177,
              "Calibration Error": 0.019,
              "Class Balance": 0.7856,
              "Object Focus": 0.8818,
              "Shape Bias": 0.2213,
              Parameters: 11.7,
          },
          
  
          
          {
              Model: "ResNet34",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#59a14f" stroke="#59a14f" stroke-width="1" opacity="0.7"/></svg> Bcos models',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "B-cos Networks: Alignment is All We Need for Interpretability",
              Accuracy: 0.7228,
              "Adversarial Robustness": 0.0071,
              "Corruption Robustness": 0.1516,
              "OOD Robustness": 0.4637,
              "Calibration Error": 0.0121,
              "Class Balance": 0.7437,
              "Object Focus": 0.8768,
              "Shape Bias": 0.2964,
              Parameters: 21.8,
          },
          
  
          
          {
              Model: "ResNet50",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#59a14f" stroke="#59a14f" stroke-width="1" opacity="0.7"/></svg> Bcos models',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "B-cos Networks: Alignment is All We Need for Interpretability",
              Accuracy: 0.7589,
              "Adversarial Robustness": 0.0095,
              "Corruption Robustness": 0.1358,
              "OOD Robustness": 0.4068,
              "Calibration Error": 0.0208,
              "Class Balance": 0.7749,
              "Object Focus": 0.9163,
              "Shape Bias": 0.2479,
              Parameters: 25.5,
          },
          
  
          
          {
              Model: "ViT-b/16",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#59a14f" stroke="#59a14f" stroke-width="1" opacity="0.7"/></svg> Bcos models',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "B-cos Networks: Alignment is All We Need for Interpretability",
              Accuracy: 0.7441,
              "Adversarial Robustness": 0.0088,
              "Corruption Robustness": 0.6646,
              "OOD Robustness": 0.546,
              "Calibration Error": 0.0042,
              "Class Balance": 0.7415,
              "Object Focus": 0.8895,
              "Shape Bias": 0.4359,
              Parameters: 86.9,
          },
          
  
          
          {
              Model: "RegNet-y-4gf",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#4e79a7" stroke="#4e79a7" stroke-width="1" opacity="0.7"/></svg> CNN',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "Designing Network Design Spaces",
              Accuracy: 0.7923,
              "Adversarial Robustness": 0.1687,
              "Corruption Robustness": 0.0236,
              "OOD Robustness": 0.2751,
              "Calibration Error": 0.0015,
              "Class Balance": 0.7581,
              "Object Focus": 0.8969,
              "Shape Bias": 0.2464,
              Parameters: 20.6,
          },
          
  
          
          {
              Model: "MobileCLIP-B (LT)",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training",
              Accuracy: 0.7725,
              "Adversarial Robustness": 0.0143,
              "Corruption Robustness": 0.7862,
              "OOD Robustness": 1.0962,
              "Calibration Error": 0.0375,
              "Class Balance": 0.8974,
              "Object Focus": 0.9511,
              "Shape Bias": 0.623,
              Parameters: 149.8,
          },
          
  
          
          {
              Model: "ViT-s/16-DINO",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "",
              Title: "Emerging Properties in Self-Supervised Vision Transformers",
              Accuracy: 0.769,
              "Adversarial Robustness": 0.0912,
              "Corruption Robustness": 0.5977,
              "OOD Robustness": 0.3993,
              "Calibration Error": 0.0012,
              "Class Balance": 0.7612,
              "Object Focus": 0.914,
              "Shape Bias": 0.289,
              Parameters: 23.2,
          },
          
  
          
          {
              Model: "SigLIP-l/16",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "Sigmoid Loss for Language Image Pre-Training",
              Accuracy: 0.8046,
              "Adversarial Robustness": 0.2293,
              "Corruption Robustness": 0.7206,
              "OOD Robustness": 1.0366,
              "Calibration Error": 0.0384,
              "Class Balance": 0.9047,
              "Object Focus": 0.9656,
              "Shape Bias": 0.6274,
              Parameters: 652.2,
          },
          
  
          
          {
              Model: "ResNet101",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#59a14f" stroke="#59a14f" stroke-width="1" opacity="0.7"/></svg> Bcos models',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ† Supervised learning",
              Text: "",
              Title: "B-cos Networks: Alignment is All We Need for Interpretability",
              Accuracy: 0.7653,
              "Adversarial Robustness": 0.0017,
              "Corruption Robustness": 0.1719,
              "OOD Robustness": 0.4918,
              "Calibration Error": 0.0042,
              "Class Balance": 0.7348,
              "Object Focus": 0.9315,
              "Shape Bias": 0.284,
              Parameters: 44.5,
          },
          
  
          
          {
              Model: "MetaCLIP-B/16",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "Demystifying CLIP Data",
              Accuracy: 0.7213,
              "Adversarial Robustness": 0.0314,
              "Corruption Robustness": 0.6825,
              "OOD Robustness": 1.0874,
              "Calibration Error": 0.0365,
              "Class Balance": 0.8913,
              "Object Focus": 0.9196,
              "Shape Bias": 0.6369,
              Parameters: 149.6,
          },
          
  
          
          {
              Model: "CLIP-ConvNeXt-L",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "LAION-5B: An open large-scale dataset for training next generation image-text models, Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
              Accuracy: 0.759,
              "Adversarial Robustness": 0.0804,
              "Corruption Robustness": 0.738,
              "OOD Robustness": 1.0629,
              "Calibration Error": 0.037,
              "Class Balance": 0.8963,
              "Object Focus": 0.9437,
              "Shape Bias": 0.572,
              Parameters: 351.8,
          },
          
  
          
          {
              Model: "MetaCLIP-L/14",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "Demystifying CLIP Data",
              Accuracy: 0.7917,
              "Adversarial Robustness": 0.0892,
              "Corruption Robustness": 0.8011,
              "OOD Robustness": 1.0727,
              "Calibration Error": 0.0382,
              "Class Balance": 0.8969,
              "Object Focus": 0.9666,
              "Shape Bias": 0.6835,
              Parameters: 427.6,
          },
          
  
          
          {
              Model: "ViT-S",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚óè Adversarial training",
              Text: "",
              Title: "Revisiting Adversarial Training for {ImageNet}: Architectures, Training and Generalization across Threat Models",
              Accuracy: 0.7279,
              "Adversarial Robustness": 0.4876,
              "Corruption Robustness": 0.6263,
              "OOD Robustness": 0.7161,
              "Calibration Error": 0.0081,
              "Class Balance": 0.7832,
              "Object Focus": 0.9393,
              "Shape Bias": 0.7203,
              Parameters: 22.8,
          },
          
  
          
          {
              Model: "CLIP-ConvNeXt-B-320px",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "LAION-5B: An open large-scale dataset for training next generation image-text models, Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
              Accuracy: 0.7128,
              "Adversarial Robustness": 0.0511,
              "Corruption Robustness": 0.6398,
              "OOD Robustness": 1.0494,
              "Calibration Error": 0.0356,
              "Class Balance": 0.8914,
              "Object Focus": 0.9238,
              "Shape Bias": 0.4906,
              Parameters: 179.4,
          },
          
  
          
          {
              Model: "CLIP-ConvNeXt-L-320px",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "LAION-5B: An open large-scale dataset for training next generation image-text models, Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
              Accuracy: 0.7659,
              "Adversarial Robustness": 0.0809,
              "Corruption Robustness": 0.7352,
              "OOD Robustness": 1.0437,
              "Calibration Error": 0.0371,
              "Class Balance": 0.8948,
              "Object Focus": 0.9497,
              "Shape Bias": 0.5477,
              Parameters: 351.8,
          },
          
  
          
          {
              Model: "Hiera-S",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "",
              Title: "Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles",
              Accuracy: 0.5532,
              "Adversarial Robustness": 0.0395,
              "Corruption Robustness": 0.5307,
              "OOD Robustness": 0.5478,
              "Calibration Error": 0.0053,
              "Class Balance": 0.7633,
              "Object Focus": 0.7871,
              "Shape Bias": 0.4726,
              Parameters: 35.0,
          },
          
  
          
          {
              Model: "Hiera-T",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ImageNet-1k",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "",
              Title: "Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles",
              Accuracy: 0.5675,
              "Adversarial Robustness": 0.0338,
              "Corruption Robustness": 0.4719,
              "OOD Robustness": 0.5617,
              "Calibration Error": 0.0044,
              "Class Balance": 0.7594,
              "Object Focus": 0.7842,
              "Shape Bias": 0.3923,
              Parameters: 27.9,
          },
  
          {
              Model: "CLIP-B16-DataCompXL",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "DataComp: In search of the next generation of multimodal datasets",
              Accuracy: 0.7347,
              "Adversarial Robustness": 0.0318,
              "Corruption Robustness": 0.6643,
              "OOD Robustness": 1.073,
              "Calibration Error": 0.0367,
              "Class Balance": 0.8963,
              "Object Focus": 0.9311,
              "Shape Bias": 0.5505,
              Size: 149.6,
          },
          
  
          
          {
              Model: "CLIP-B16-Laion2B",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "LAION-5B: An open large-scale dataset for training next generation image-text models, Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
              Accuracy: 0.7022,
              "Adversarial Robustness": 0.0164,
              "Corruption Robustness": 0.6246,
              "OOD Robustness": 1.0764,
              "Calibration Error": 0.0357,
              "Class Balance": 0.892,
              "Object Focus": 0.9153,
              "Shape Bias": 0.506,
              Size: 149.6,
          },
          
  
          
          {
              Model: "CLIP-B16-CommonPool-XL-DFN2B",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "Data Filtering Networks",
              Accuracy: 0.7625,
              "Adversarial Robustness": 0.0401,
              "Corruption Robustness": 0.6555,
              "OOD Robustness": 1.027,
              "Calibration Error": 0.0374,
              "Class Balance": 0.9024,
              "Object Focus": 0.9369,
              "Shape Bias": 0.5429,
              Size: 149.6,
          },
          
  
          
          {
              Model: "CLIP-B16-OpenAI-FT-Vision-Encoder",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "Reproducible scaling laws for contrastive language-image learning",
              Accuracy: 0.8529,
              "Adversarial Robustness": 0.1696,
              "Corruption Robustness": 0.7216,
              "OOD Robustness": 0.8213,
              "Calibration Error": 0.0024,
              "Class Balance": 0.8059,
              "Object Focus": 0.9436,
              "Shape Bias": 0.3445,
              Size: 86.6,
          },
          
  
          
          {
              Model: "CLIP-B16-Laion2B-FT-Vision-Encoder",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "Reproducible scaling laws for contrastive language-image learning",
              Accuracy: 0.8547,
              "Adversarial Robustness": 0.142,
              "Corruption Robustness": 0.7374,
              "OOD Robustness": 0.8695,
              "Calibration Error": 0.0027,
              "Class Balance": 0.8086,
              "Object Focus": 0.9454,
              "Shape Bias": 0.3796,
              Size: 86.6,
          },
          
  
          
          {
              Model: "CLIP-B32-OpenAI-FT-Vision-Encoder",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "Reproducible scaling laws for contrastive language-image learning",
              Accuracy: 0.8192,
              "Adversarial Robustness": 0.168,
              "Corruption Robustness": 0.7003,
              "OOD Robustness": 0.7968,
              "Calibration Error": 0.002,
              "Class Balance": 0.7886,
              "Object Focus": 0.925,
              "Shape Bias": 0.4007,
              Size: 88.2,
          },
          
  
          
          {
              Model: "CLIP-B32-Laion2B-FT-Vision-Encoder",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "Reproducible scaling laws for contrastive language-image learning",
              Accuracy: 0.8259,
              "Adversarial Robustness": 0.1657,
              "Corruption Robustness": 0.6961,
              "OOD Robustness": 0.8182,
              "Calibration Error": 0.002,
              "Class Balance": 0.7919,
              "Object Focus": 0.9306,
              "Shape Bias": 0.4534,
              Size: 88.2,
          },
          
  
          
          {
              Model: "CLIP-L14-OpenAI",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "Learning Transferable Visual Models From Natural Language Supervision",
              Accuracy: 0.7555,
              "Adversarial Robustness": 0.3235,
              "Corruption Robustness": 0.755,
              "OOD Robustness": 1.0365,
              "Calibration Error": 0.011,
              "Class Balance": 0.8909,
              "Object Focus": 0.9438,
              "Shape Bias": 0.5991,
              Size: 427.6,
          },
          
  
          
          {
              Model: "CLIP-L14-DataCompXL",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "Reproducible scaling laws for contrastive language-image learning",
              Accuracy: 0.7919,
              "Adversarial Robustness": 0.069,
              "Corruption Robustness": 0.7785,
              "OOD Robustness": 1.0521,
              "Calibration Error": 0.038,
              "Class Balance": 0.9022,
              "Object Focus": 0.9635,
              "Shape Bias": 0.6066,
              Size: 427.6,
          },
          
  
          
          {
              Model: "CLIP-L14-Laion2B",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "LAION-5B: An open large-scale dataset for training next generation image-text models, Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
              Accuracy: 0.7525,
              "Adversarial Robustness": 0.0138,
              "Corruption Robustness": 0.7052,
              "OOD Robustness": 1.0694,
              "Calibration Error": 0.037,
              "Class Balance": 0.8944,
              "Object Focus": 0.9531,
              "Shape Bias": 0.5405,
              Size: 427.6,
          },
          
  
          
          {
              Model: "CLIP-L14-CommPool XL-DFN2B",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "Data Filtering Networks",
              Accuracy: 0.8139,
              "Adversarial Robustness": 0.1274,
              "Corruption Robustness": 0.772,
              "OOD Robustness": 1.0378,
              "Calibration Error": 0.0386,
              "Class Balance": 0.9095,
              "Object Focus": 0.9572,
              "Shape Bias": 0.5929,
              Size: 427.6,
          },
          
  
          
          {
              Model: "ViT-L/14-DINOv2-reg-LP",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "Vision Transformers Need Registers",
              Accuracy: 0.8674,
              "Adversarial Robustness": 0.2146,
              "Corruption Robustness": 0.8824,
              "OOD Robustness": 0.875,
              "Calibration Error": 0.001,
              "Class Balance": 0.811,
              "Object Focus": 0.9425,
              "Shape Bias": 0.6169,
              Size: 309.5,
          },
          
  
          
          {
              Model: "ViT-B/14-DINOv2-reg-LP",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "Vision Transformers Need Registers",
              Accuracy: 0.8456,
              "Adversarial Robustness": 0.1197,
              "Corruption Robustness": 0.7927,
              "OOD Robustness": 0.791,
              "Calibration Error": 0.0011,
              "Class Balance": 0.7972,
              "Object Focus": 0.941,
              "Shape Bias": 0.4861,
              Size: 90.4,
          },
          
  
          
          {
              Model: "ViT-S/14-DINOv2-reg-LP",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "Vision Transformers Need Registers",
              Accuracy: 0.8089,
              "Adversarial Robustness": 0.0623,
              "Corruption Robustness": 0.6761,
              "OOD Robustness": 0.7105,
              "Calibration Error": 0.0011,
              "Class Balance": 0.7789,
              "Object Focus": 0.9289,
              "Shape Bias": 0.3485,
              Size: 24.0,
          },
          
  
          
          {
              Model: "ViT-L/14-DINOv2-FT",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "DINOv2: Learning Robust Visual Features without Supervision",
              Accuracy: 0.8656,
              "Adversarial Robustness": 0.3172,
              "Corruption Robustness": 0.8399,
              "OOD Robustness": 0.8586,
              "Calibration Error": 0.005,
              "Class Balance": 0.8238,
              "Object Focus": 0.9726,
              "Shape Bias": 0.5068,
              Size: 309.5,
          },
          
  
          
          {
              Model: "ViT-B/14-DINOv2-FT",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "DINOv2: Learning Robust Visual Features without Supervision",
              Accuracy: 0.8538,
              "Adversarial Robustness": 0.2192,
              "Corruption Robustness": 0.7948,
              "OOD Robustness": 0.7859,
              "Calibration Error": 0.005,
              "Class Balance": 0.8139,
              "Object Focus": 0.9693,
              "Shape Bias": 0.4902,
              Size: 90.4,
          },
          
  
          
          {
              Model: "ViT-S/14-DINOv2-FT",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "DINOv2: Learning Robust Visual Features without Supervision",
              Accuracy: 0.8189,
              "Adversarial Robustness": 0.1826,
              "Corruption Robustness": 0.7192,
              "OOD Robustness": 0.6982,
              "Calibration Error": 0.0054,
              "Class Balance": 0.7952,
              "Object Focus": 0.956,
              "Shape Bias": 0.3333,
              Size: 24.0,
          },
          
  
          
          {
              Model: "ViT-L/14-DINOv2-reg-LP",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "Vision Transformers Need Registers",
              Accuracy: 0.8767,
              "Adversarial Robustness": 0.4275,
              "Corruption Robustness": 0.871,
              "OOD Robustness": 0.8962,
              "Calibration Error": 0.0044,
              "Class Balance": 0.8287,
              "Object Focus": 0.9637,
              "Shape Bias": 0.6323,
              Size: 309.5,
          },
          
  
          
          {
              Model: "ViT-B/14-DINOv2-reg-LP",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "Vision Transformers Need Registers",
              Accuracy: 0.8566,
              "Adversarial Robustness": 0.2576,
              "Corruption Robustness": 0.8059,
              "OOD Robustness": 0.8244,
              "Calibration Error": 0.0048,
              "Class Balance": 0.8143,
              "Object Focus": 0.9715,
              "Shape Bias": 0.4109,
              Size: 90.4,
          },
          
  
          
          {
              Model: "ViT-S/14-DINOv2-reg-LP",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#f28e2b" stroke="#f28e2b" stroke-width="1" opacity="0.7"/></svg> Transformer',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "Vision Transformers Need Registers",
              Accuracy: 0.8105,
              "Adversarial Robustness": 0.1496,
              "Corruption Robustness": 0.7248,
              "OOD Robustness": 0.7048,
              "Calibration Error": 0.0052,
              "Class Balance": 0.7922,
              "Object Focus": 0.9583,
              "Shape Bias": 0.357,
              Size: 24.0,
          },
          
  
          
          {
              Model: "SigLIP2-b/16",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features",
              Accuracy: 0.7849,
              "Adversarial Robustness": 0.123,
              "Corruption Robustness": 0.6718,
              "OOD Robustness": 1.04,
              "Calibration Error": 0.038,
              "Class Balance": 0.9048,
              "Object Focus": 0.9594,
              "Shape Bias": 0.553,
              Size: 375.2,
          },  
          
          {
              Model: "SigLIP2-l/16",
              Architecture: ' <svg width="15" height="15" style="vertical-align: middle; margin-right: 5px;"><rect width="15" height="15" fill="#edc948" stroke="#edc948" stroke-width="1" opacity="0.7"/></svg> Vision-language models',
              Dataset: " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)",
              Setup: " ‚ñ≤ Self-supervised learning",
              Text: "‚òÖ",
              Title: "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features",
              Accuracy: 0.8235,
              "Adversarial Robustness": 0.3231,
              "Corruption Robustness": 0.8046,
              "OOD Robustness": 1.0571,
              "Calibration Error": 0.0389,
              "Class Balance": 0.9072,
              "Object Focus": 0.9769,
              "Shape Bias": 0.6881,
              Size: 881.5,
          },
  
              
  
              ];
              
              setData(sampleData);
          }, []);
  
          const updatePlot = () => {
              const filteredData = data.filter(d => 
                  filters.architectures.includes(d.Architecture) &&
                  filters.datasets.includes(d.Dataset) &&
                  filters.training.includes(d.Setup)
              );
  
              const trace = {
                  x: filteredData.map(d => d[filters.xAxis]),
                  y: filteredData.map(d => d[filters.yAxis]),
                  mode: 'markers+text',
                  type: 'scatter',
                  text: filteredData.map(d => d.Text),
                  textposition: 'middle center',
                  textfont: {
                      size: 8,  
                      color: 'black'  
                  },
                  marker: {
                      symbol: filteredData.map(d => SHAPE_MAP[d.Setup]),
                      color: filteredData.map(d => COLOR_MAP[d.Architecture]),
                      size: 17,
                      opacity: 0.7
                  },
                  hovertemplate: 
                      '<b>%{customdata[0]}</b><br>' +
                      'Model: %{customdata[1]}<br>' +
                      'Acc.: %{customdata[2]:.4f}<br>' +
                      'Adv. Rob.: %{customdata[3]:.4f}<br>' +
                      'C-Rob.: %{customdata[4]:.4f}<br>' +
                      'OOD Rob.: %{customdata[5]:.4f}<br>' +
                      'Cal. Err.: %{customdata[6]:.4f}<br>' +
                      '"Class Balance": %{customdata[7]:.4f}<br>' +
                      'Object Focus: %{customdata[8]:.4f}<br>' +
                      'Shape Bias: %{customdata[9]:.4f}<br>' +
                      'Parameters: %{customdata[10]}<br>' +
                      'Dataset: %{customdata[11]}<br>' +
                      'Setup: %{customdata[12]}<br>' +
                      '<extra></extra>',
                  customdata: filteredData.map(d => [
                      d.Title,
                      d.Model,
                      d.Accuracy,
                      d["Adversarial Robustness"],
                      d["Corruption Robustness"],
                      d["OOD Robustness"],
                      d["Calibration Error"],
                      d["Class Balance"],
                      d["Object Focus"],
                      d["Shape Bias"],
                      d.Parameters,
                      d.Dataset,
                      d.Setup
                  ])
              };
  
              const layout = {
                  xaxis: { 
                      title: filters.xAxis, 
                      range: AXIS_RANGES[filters.xAxis],              
                  },
                  yaxis: { 
                      title: filters.yAxis, 
                      range: AXIS_RANGES[filters.yAxis],
                  },
                  showlegend: false,
                  hovermode: 'closest',
                  responsive: true,
                  autosize: true,
              };
          
              Plotly.newPlot('plot', [trace], layout);
              window.addEventListener('resize', function() { Plotly.relayout('plot', { autosize: true }); }); 
          };
  
          React.useEffect(() => {
              updatePlot();
          }, [data, filters]);


          const CheckboxGroup = ({ title, options, selected, onChange }) => {
              return e('div', { className: 'control-group' }, [
                  e('label', null, title),
                  e('div', { className: 'checkbox-group' },
                      options.map(option => 
                          e('label', { className: 'checkbox-label', key: option }, [
                              e('input', {
                                  type: 'checkbox',
                                  checked: selected.includes(option),
                                  onChange: (event) => {
                                      if (event.target.checked) {
                                          onChange([...selected, option]);
                                      } else {
                                          onChange(selected.filter(item => item !== option));
                                      }
                                  }
                              }),
                              e('span', { 
                                  dangerouslySetInnerHTML: { __html: option } 
                              })
                          ])
                      )
                  )
              ]);
          };
  
          const RadioGroup = ({ title, options, selected, onChange }) => {
              return e('div', { className: 'control-group' }, [
                  e('label', null, title),
                  e('div', { className: 'radio-group' },
                      options.map(option => 
                          e('label', { className: 'checkbox-label', key: option }, [
                              e('input', {
                                  type: 'radio',
                                  name: title,
                                  value: option,
                                  checked: selected === option,
                                  onChange: (event) => onChange(event.target.value)
                              }),
                              option
                          ])
                      )
                  )
              ]);
          };
  
  
  
          const InteractiveTable = ({ data, filters }) => {
              const [sortConfig, setSortConfig] = React.useState({
                  key: null,
                  direction: 'ascending'
              });
              
              // Filter data based on the current filters
              const filteredData = data.filter(d => 
                  filters.architectures.includes(d.Architecture) &&
                  filters.datasets.includes(d.Dataset) &&
                  filters.training.includes(d.Setup)
              );
              
              // Apply sorting if configured
              const sortedData = React.useMemo(() => {
                  let sortableData = [...filteredData];
                  if (sortConfig.key) {
                      sortableData.sort((a, b) => {
                          if (a[sortConfig.key] < b[sortConfig.key]) {
                              return sortConfig.direction === 'ascending' ? -1 : 1;
                          }
                          if (a[sortConfig.key] > b[sortConfig.key]) {
                              return sortConfig.direction === 'ascending' ? 1 : -1;
                          }
                          return 0;
                      });
                  }
                  return sortableData;
              }, [filteredData, sortConfig]);
              
              const requestSort = (key) => {
                  let direction = 'ascending';
                  if (sortConfig.key === key && sortConfig.direction === 'ascending') {
                      direction = 'descending';
                  }
                  setSortConfig({ key, direction });
              };
              
              const getClassNamesFor = (name) => {
                  if (!sortConfig) {
                      return;
                  }
                  return sortConfig.key === name ? 
                      sortConfig.direction === 'ascending' ? 'sort-asc' : 'sort-desc' 
                      : undefined;
              };
              
              // Get the symbol based on the Setup
              const getSymbol = (setup) => {
                  const symbols = {
                      " ‚ñ† Supervised learning": "‚ñ† SL",
                      " ‚óè Adversarial training": "‚óè AT",
                      " ‚ñ≤ Self-supervised learning": "‚ñ≤ self-SL",
                      " ‚óÜ Semi-supervised learning": "‚óÜ semi-SL",
                      " ‚¨ü A[1,2,3]": "‚¨ü A[1,2,3]"
                  };
                  return symbols[setup] || "";
              };
  
              const getDataset = (setup) => {
                  const symbols = {
                      " ImageNet-1k": "IN1k",
                      " ‚óè ImageNet-21k": "‚óè IN21k",
                      " ‚òÖ Large Dataset (Laion2B, JFT-300M, ...)": "‚òÖ LD",
                  };
                  return symbols[setup] || "";
              };
              
              // Get color block based on architecture
              const getArchitectureColor = (architecture) => {
                  // Extract the color from the architecture string
                  const colorMatch = architecture.match(/#[0-9a-f]{6}/i);
                  if (colorMatch) {
                      const color = colorMatch[0];
                      return e('div', {
                          style: {
                              width: '15px',
                              height: '15px',
                              backgroundColor: color,
                              display: 'inline-block',
                              marginRight: '5px',
                              verticalAlign: 'middle'
                          }
                      });
                  }
                  return null;
              };
              
              // Extract architecture name without SVG
              const getArchitectureName = (architecture) => {
                  return architecture.replace(/<svg.*?<\/svg>/, '').trim();
              };
              
              return e('div', { className: 'table-container', key: 'table-section', style: { position: 'relative' }  }, [
  
  
              e('div', { style: { display: 'flex', justifyContent: 'space-between', marginBottom: '10px' } }, [
                  
                  e('button', {
                      className: 'reset-button',
                      onClick: () => setSortConfig({ key: null, direction: 'ascending' }),
                      style: {
                          backgroundColor: 'red',
                          color: 'white',
                          padding: '5px 15px', 
                          borderRadius: '10px',
                          border: 'none',
                          cursor: 'pointer',
                      }
                  }, 'Reset Sorting')
              ]),
  
                  
  
                  e('table', { className: 'data-table' }, [
  
                  e('thead', null, 
                          e('tr', null, [
                              e('th', { onClick: () => requestSort('Model'), className: getClassNamesFor('Model')} , 'Model'),
                              e('th', { onClick: () => requestSort('Architecture'), className: getClassNamesFor('Architecture')}, 'Arch.'),
                              e('th', { onClick: () => requestSort('Setup'), className: getClassNamesFor('Setup') }, 'Setup'),
                              e('th', { onClick: () => requestSort('Dataset'), className: getClassNamesFor('Dataset')}, 'Dataset'),
                              e('th', { onClick: () => requestSort('Accuracy'), className: getClassNamesFor('Accuracy') }, 'Accuracy'),
                              e('th', { onClick: () => requestSort('Adversarial Robustness'), className: getClassNamesFor('Adversarial Robustness') }, 'Adv. Rob.'),
                              e('th', { onClick: () => requestSort('Corruption Robustness'), className: getClassNamesFor('Corruption Robustness') }, 'Corr. Rob.'),
                              e('th', { onClick: () => requestSort('OOD Robustness'), className: getClassNamesFor('OOD Robustness') }, 'OOD Rob.'),
                              e('th', { onClick: () => requestSort('Calibration Error'), className: getClassNamesFor('Calibration Error') }, 'Calib. Error'),
                              e('th', { onClick: () => requestSort('Class Balance'), className: getClassNamesFor('"Class Balance"') }, 'Class Balance'),
                              e('th', { onClick: () => requestSort('Object Focus'), className: getClassNamesFor('Object Focus') }, 'Object Focus'),
                              e('th', { onClick: () => requestSort('Shape Bias'), className: getClassNamesFor('Shape Bias') }, 'Shape Bias'),
                              e('th', { onClick: () => requestSort('Parameters'), className: getClassNamesFor('Parameters') }, 'Params')
                          ])
                      ),
                      e('tbody', null, 
                          sortedData.map((item, index) => 
                              e('tr', { key: index }, [
                                  e('td', null, item.Model),
                                  e('td', null, [
                                  e('div', { style: { display: 'flex', alignItems: 'center' } }, [
                                      getArchitectureColor(item.Architecture),
                                      e('span', { style: { marginLeft: '5px' } }, getArchitectureName(item.Architecture))
                                  ])
                              ]),
                                  e('td', null, getSymbol(item.Setup)),
                                  e('td', null, getDataset(item.Dataset)),
                                  e('td', null, item.Accuracy.toFixed(4)),
                                  e('td', null, item['Adversarial Robustness'].toFixed(4)),
                                  e('td', null, item['Corruption Robustness'].toFixed(4)),
                                  e('td', null, item['OOD Robustness'].toFixed(4)),
                                  e('td', null, item['Calibration Error'].toFixed(4)),
                                  e('td', null, item["Class Balance"].toFixed(4)),
                                  e('td', null, item['Object Focus'].toFixed(4)),
                                  e('td', null, item['Shape Bias'].toFixed(4)),
                                  e('td', null, item.Parameters)
                              ])
                          )
                      )
                  ]),
  
              ]);
          };
  
          const { useState } = React;


            function HideableTable({ data, filters }) {
                const [isTableVisible, setIsTableVisible] = useState(false);

                return e('div', null,

                    e('button', { 
                        onClick: () => setIsTableVisible(!isTableVisible),
                        style: {
                            padding: '10px 20px',       
                            fontSize: '16px',        
                            fontWeight: 'bold',       
                            backgroundColor: '#007bff',
                            color: 'white',    
                            border: 'none',         
                            borderRadius: '5px',        
                            cursor: 'pointer'           
                        }
                    }, isTableVisible ? 'Hide Table' : 'Show Table'),

                    isTableVisible && e(InteractiveTable, { data, filters })
                );
            }

          return e('div', { className: 'controls-container' }, [
              
              
              // Plot section
              e('div', { className: 'plot-container', key: 'plot-container', style:{display: 'flex', justifyContent: 'center', alignItems: 'center' } }, 
                  e('div', { id: 'plot', key: 'plot'})
              ),
              
  
              // Controls section
              e('div', { className: 'controls', key: 'controls' }, [
                  e(CheckboxGroup, {
                      key: 'arch',
                      title: 'Architectures:',
                      options: ALL_ARCHITECTURES,
                      selected: filters.architectures,
                      onChange: (value) => setFilters(prev => ({ ...prev, architectures: value }))
                  }),
                  e(CheckboxGroup, {
                      key: 'dataset',
                      title: 'Datasets:',
                      options: ALL_DATASETS,
                      selected: filters.datasets,
                      onChange: (value) => setFilters(prev => ({ ...prev, datasets: value }))
                  }),
                  e(CheckboxGroup, {
                      key: 'training',
                      title: 'Training:',
                      options: ALL_TRAINING,
                      selected: filters.training,
                      onChange: (value) => setFilters(prev => ({ ...prev, training: value }))
                  }),
                  e(RadioGroup, {
                      key: 'xAxis',
                      title: 'X-Axis:',
                      options: ALL_METRICS,
                      selected: filters.xAxis,
                      onChange: (value) => setFilters(prev => ({ ...prev, xAxis: value }))
                  }),
                  e(RadioGroup, {
                      key: 'yAxis',
                      title: 'Y-Axis:',
                      options: ALL_METRICS,
                      selected: filters.yAxis,
                      onChange: (value) => setFilters(prev => ({ ...prev, yAxis: value }))
                  })
              ]),
            
              e(HideableTable, { data, filters }),

          ]);
      };
  
      const root = ReactDOM.createRoot(document.getElementById('root'));
      root.render(e(App));
    </script>

    <div style="text-align: 'left'; width: '100%' ">
        <h2 style="margin-bottom: '15px'">Acknowledgements</h2>
        <div className="acknowledgements" key="acknowledgements">
            <p style="vertical-align: 'super'; font-size:18px">
            This project has received funding from the European Research Council (ERC) under the European Union‚Äôs Horizon 2020 research and innovation programme (grant agreement No. 866008). The project has also been funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) ‚Äì project number 529680848. Further, the project has been supported by the State of Hesse through the cluster projects ‚ÄúThe Third Wave of Artificial Intelligence (3AI)‚Äù and ‚ÄúThe Adaptive Mind (TAM).‚Äù
            </p>
        </div>
        </div>

        <div style="text-align: 'left'; width: '100%'">

    <h2 style="margin-bottom: '15px'">Citation</h2>

        <p style="font-size:18px">
            If you find this project useful, please consider citing:
        </p>

        <div style="background-color: #f4f4f4; padding: 15px; border-radius: 5px; overflow-x: auto;">
            <pre>
    @article{Hesse:2025:beyond_accuracy,
        title={Beyond Accuracy: What Matters in Designing Well-Behaved Models?}, 
        author={Robin Hesse and Do\u{g}ukan Ba\u{g}c\i and Bernt Schiele and Simone Schaub-Meyer and Stefan Roth},
        year={2025},
        journal={arXiv:2503.17110 [cs.CV]},
    }
            </pre>
        </div>
        
    </div>
    
    <footer style="text-align: center;">
        <p>&copy; Visual Inference Lab. Licensed under the 
            <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache License 2.0</a>.
        </p>
    </footer>

</body>
</html>